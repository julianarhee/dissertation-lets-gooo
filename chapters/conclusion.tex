\chapter{Conclusion}
\label{conclusion}

Visual object recognition is a computationally challenging problem that remains unsolved. Most studies are in humans and monkeys, which are experimentally much less tractable. In recent years, rodents have emerged as powerful systems in which to study visual circuits. 

There are increasingly sophisticated genetic tools (e.g., cell-type-specific targeting of single cells, simultaneous optogenetic manipulation and imaging) and optical imaging techniques (e.g., multi-channel imaging, multi-photon imaging, holographic stimulation) available for understanding the neural circuits associated with a given behavior. 


Even among rodents, there is a huge diversity of visual capacities and specializations, so understanding which aspects are preserved across species and which are species-specific may help elucidate fundamental principles of perceptual behavior. 

Rats exhibit complex behaviors in the field and in the lab. Training rats on complicated tasks takes a great deal of time and labor, and there is little standardization across labs. We provide a modular, low-cost, and open-source platform for training large cohorts of rats on a range of visual tasks.

Despite rats’ amazing behavioral capacities, few paradigms exist for chronic, large-scale, cellular-resolution imaging in awake rats, a class of approaches that have proved immensely powerful in dissecting neural circuits in smaller animals, such as worms, flies, and mice. While systems that combine freely moving behavior with tethered or wireless recordings are rapidly improving and highly valuable, awake, head-fixed systems are beneficial for careful stimulus delivery and behavioral monitoring, and chronic preparations are necessary for longitudinal studies.   

There are increasingly sophisticated genetic tools (e.g., cell-type-specific targeting of single cells, simultaneous optogenetic manipulation and imaging) and optical imaging techniques (e.g., multi-channel imaging, multi-photon imaging, holographic stimulation) available for understanding the neural circuits associated with a given behavior. We demonstrate the feasibility and promise of applying such approaches to awake, head-fixed rats for the first time. 

\section{Comparison with existing methods}
Typically, rats are used in freely-moving or unrestrained paradigms. Electrophysiology is commonly used in rats, and in awake animals, provides a powerful preparation for studying neural activity in the context of complex and naturalistic behaviors, as the animal need not be restrained. However, chronic access to the same cells across long periods of time, such as over the course of learning, is challenging and difficult to verify. Furthermore, although increasingly higher-density probes may improve the resolution of recording, they can damage neural tissue, and fine-scale, cell-type-specific manipulations are much harder to control (as opposed to, say, holographic stimulation).  

For optical imaging, the larger size of the rat has been advantageous, as head-mounted scopes can be affixed to the skull for chronic, long-term imaging. However, there is usually tradeoff between having a small FOV with cellular resolution (head-mountable 2p, REFREF) or a large FOV without cellular resolution (single-photon, REFREF). Moreover, in order to access lateral cortex on the side of the animal’s head, head-mounted preparations are difficult to maintain in a stable way. Scott et al., (2013) successfully demonstrated the feasibility of voluntary head-fixation for cellular resolution imaging in trained rats. However, this involves time-intensive and laborious training, and data acquisition is less under experimenter control. 

Our study overcomes many of these tradeoffs and demonstrates the feasibility of high-resolution, large-FOV imaging with cellular resolution in awake, head-fixed rats. We leverage the head-fixed system to simultaneously track high-resolution video of the animal’s facial movements and behaviors. Although head-fixing the animal comes at the cost of studying neural activity under unnatural conditions, it comes with the benefit of minimizing highly complex behaviors and actions that are irrelevant to the stimuli being investigated. Specifically, this integrated approach allows us to investigate visual object representations in the context of different states of arousals. Typically, animals are not engaged in a fixed position, let alone a fixed behavior state, when interacting with objects in the world. As such, it is important to understand how different behavior states and task conditions modulate the extent to which neural representations can or cannot support complex behavior such as invariant object recognition.


\section{Comparison with previous studies in rodents}
Vermaerke \textit{et al.} (2014) used electrophysiology to record from awake, head-fixed, naive rats, similar to our study. They found that linear decoding accuracy for static images (simple shapes presented at a given position) were best in V1, while LM and LI accuracies were barely above chance. In contrast, matching for the same pseudo-population sizes (N=64 cells), we found discriminability to be comparable across all areas under study (V1, LM, and LI) and well above chance. This could be due to differences in stimuli, as they use simple stimuli composed of oriented edges (e.g., triangles, squares), while we use slightly more complex, pseudo-3D object stimuli.

Tafazoli \textit{et al.} (2017) also recorded in naive rats with electrophysiology, but used stimuli similar to ours and recorded in anesthetized, head-fixed rats. Overall, they found significant improvements in all metrics tested for area LL, which was not included in our study. However, their findings in areas V1, LM, and LI are consistent with our results. They found that across areas V1, LM and LI, classifier accuracies on tests for linear separability and generalization were comparable, though LI performance was significantly greater than that of V1 and LM. This is consistent with our finding that, of the three areas, LI appears the most robust to generalization tests across size.

Interestingly, they also found that LM accuracies were the worst overall, ~5-8\% lower classifier accuracy than V1 and LI. We find a similar pattern of results, with LM accuracy ~8\% lower than V1 and LI) when matching for pseudo-population size (n=96 cells), as does Vermaerke \textit{et al.} (2014), with LM performing the worst in discriminating static images. Absolute accuracy levels were higher in our study and Vermaerke et al., compared to accuracy levels reported for a subset of conditions by Tafazoli et al. (pseudo-population, n=96 cells). This could be due to differences in behavioral state (Tafazoli et al. recorded in anesthetized rats) or differences in stimuli (Tafazoli et al average across a much more diverse and complex array of object images). 

\section{Comparison with other species}
Froudarakis \textit{et al.} (2020) is the only study to show that mice may be capable of the type of object recognition tasks used to train rats. Although overall performance in mice falls short of the accuracy levels we find in our trained rats, generalization appears robust. Their study provides a valuable point of comparison as they also record in awake, passively-viewing animals using two-photon imaging.

Unlike most rat studies, they used dynamic movie stimuli as opposed to static images while recording in awake, head-fixed mice. Like Vermaerke et al., who also used dynamic movie stimuli, Froudarakis et al. found that discriminability increases from V1 to LM to LI at the level of single neurons -- these single unit results are similar to those of Vermaerke et al. (2014), who found increasing discriminability by another metric of shape selectivity, i.e., sparseness. However, at the population level, the ephys rat studies and imaging mouse study appear to differ, with highest discriminability in LM in the mouse (and area AL, not tested here), and in rats, the worst performance across all metrics for area LM. Froudarakis et al. also find an increase in discriminability as a function of stimulus size. These results are consistent with ours, but only for areas V1 and LM. In area LI, we find that discriminability, as measured by classifier accuracy, is fairly constant across the range of sizes tested. 

Both Froudarakis et al. and Tafazoli et al. tested multiple types of identity-preserving transformations, and consistent with our results, that LI appears more robust to size transformations, both studies find that LI tends to exhibit some improvement beyond V1, depending on the particular transformation. Interestingly, Froudarakis et al. find LM and AL to be the most robust to transformations, whereas this is area LL for Tafazoli et al. Area LL is a region lateral to LI in rats, but not tested in Froudarakis et al, so future studies may help identify additional extrastriate areas that exhibit features thought to be important for visual object recognition.  

\section{Caveats}
Compared to previous studies of tolerant object representations in rodent visual cortex, our study presents a limited set of shapes and transformations. As such, it is difficult to identify the extent to which our results are generalizable across different types of transformations. Given that previous studies have shown that there are asymmetries across transformations in separability and generalization, future work should investigate the degree of tolerant representations a given visual area can support. 

Although we imaged in awake, head-fixed rats, our recordings were in naive, passively viewing animals. Given the fact vision appears to be less of a dominant sense in rodents than it is in monkeys, presenting visual stimuli that have acquired behavioral importance through training may result in significant differences than has been shown in any rodent model. However, many of the studies characterizing the hierarchical organization of primate visual cortex have relied on naive monkeys, either passively viewing or engaged in a simple, orthogonal task\cite{REFREF}. As such, recording in naive rodents allows a valuable point of comparison between the species, as they likely experience dramatically different visual statistics during development.

\section{Future Directions}
Currently, mice and flies are the most commonly used models in studies that leverage the power of cellular resolution imaging and genetic tools. Although rats are an equally popular model system across many fields of neuroscience, such techniques have remained challenging to apply to larger rodent models. Future studies may leverage the developments and findings of the present study to combine training behavior with two-photon imaging. Many of the existing genetic tools used in mice are readily applicable to rats, and other rodents such as transgenic rats that express GCaMP pan-neuronally or in cell-type specific populations. Combined with multi-channel systems, as presented here, fine-scale circuit dissection and manipulation is readily available for future studies in rats. 

One of the most exciting avenues in systems neuroscience research today is the ability to study neural circuits simultaneously with behavior over the course of learning. Despite rats being the standard model for studies of learning and memory, few studies have been able to track the same cells over time in rats learning these complex tasks. Over a decade ago, Zoccolan et al. (2009) demonstrated that rats are capable of invariant object recognition, a behavior that was long considered to be unique to primates. Ten years later, Froudarakis et al have shown that mice, too, are capable of this complex visual behavior. Although a great deal has been learned about how a biological system solves this problem, many questions remain that are unfeasible to ask in primates, and rodent models offer a promising alternative by which both general principles and species-specific adaptations can be discovered. 

Anatomically, we have known that rodent visual cortex has a hierarchical organization (CITE REFREF). Over the past decade, a growing number of studies have started to parse out possible functional hierarchies within rodent visual cortex (CITE REFREF). At present, it remains unclear whether rodent visual areas neatly map onto primate visual areas. In the context of visual object recognition, rodent visual cortex seems less starkly differentiated, although broad principles, such as increasing view tolerance and object discriminability are beginning to emerge. However, investigations into rodents as a model for what is traditionally considered “high-level” vision have only just begun. Our study closes a long-standing gap to unlock the rat as a powerful model system for systems neuroscience. 

The technological advancements presented in this work have the potential to unlock a wide range of new experimental directions, enabled by the superior molecular and genetic tools available in rodents. With the groundwork laid out in the previous chapters, we overcome significant barriers to position future studies to ask new, fundamental questions about the nature of visual population representations in cortex, how they change with experience, and how they might be read-out by downstream cortical areas. By approaching these questions with a technique that allows the measurement of responses from a large population of neurons tracked across days, weeks, and months, it is possible to ask new questions about the nature of population representations in cortex that have been inaccessible for decades.

This work sits at the vanguard of a growing trend of using rodent models to address questions more traditionally explored in non-human primates.  The cognitive capabilities of rodents have been consistently underestimated, but increasingly, research groups have successfully challenged the notion that high level processes (such as decision making) can only be studied in primates. This project opens the way for future studies to ask new, fundamental questions about cortex that were previously inaccessible using other experimental means. 
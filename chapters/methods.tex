\chapter{Methods}
\label{methods}

% Materials and methods
% ----------------------------------------------------
\section{Animals}
All experimental procedures were reviewed and approved by the Harvard Institutional Animal Care and Use Committee. Experiments were performed at Harvard University. Animals used in this study were female Long-Evans rats, 3 months or older, weighing 250-350g (Charles River Laboratories). Rats were housed on a ventilated rack under a reverse 12 hour light:dark cycle with food and water ad libitum, except when water-restricted for behavior training. 

% ----------------------------------------------------
% Behavior
% ----------------------------------------------------
\section{Visual Behavior}
\subsection{Subjects}
 A total of N=56 rats were trained on the basic three-port task. For testing the throughput and reliability of the training apparatus (OpenRatBox), the training data from a total of 32 animals were analyzed. Of these, 13 animals were tested for each of the generalization tests of identity-preserving transformations and identity-changing transformations. Rats were kept on a water schedule in which they received the majority of their water during behavior training sessions. Rats were given ad lib water for 1 hr if few (<100) trials were performed. 
 
\subsection{Training Apparatus}
All animals were trained using OpenRatBox (see Figure\ref{fig:openratbox}). The frame of the box was composed of custom-cut aluminum extrusions (80/20, Inc.). Each training box housed a clear, plastic home-cage along with all behavior paraphernalia. Four training boxes were vertically stacked into one tower, with a total of four towers included for the present study. Although rats could be kept in the plastic housing cages for home-cage training, the current study only used the boxes for training sessions. For each session, rats were individually transferred from the colony to the plastic housing cage in their assigned training box. The latching mechanism between the training box and housing cage was designed to ensure that the cage is snapped and locked into the same position from one session to the next. At one of the housing cage, a small port hole ($\sim$3cm diameter) allowed the animal to extend its head out to face the monitor and access the reward ports.

Each behavior box was equipped with a monitor (Dell P190S, Round Rock, TX; Samsung 943-BT, Seoul, South Korea), positioned $\sim$30 $cm$ from the animal's head, and a computer (MacMini 6, OSX 10.9.5 or MacMini 7, OSX El Capitan 10.11.13, Apple, Cupertino, CA) mounted above the main vestibule. The computer ran an MWorks server (MWorks 0.5.dev [d7c9069] or 0.6 [c186e7], The MWorks Project https://mworks.github.io/), and I/O applications for reward delivery (Arduino\textregistered, Phidget). The monitor was placed about 30cm from the rat's eyes. 

For the three-port task (see Figure\ref{fig:basic_training}), a custom mount holding three feeding tubes (Cadence 7909, 14G Straight Feeding Tube w/ 4mm diameter ball) on an aluminum frame positioned directly in front of the port hole (MicroRax). The three sensors were positioned $~$1 cm apart along a straight horizontal bar, at a height matching the rat's mouth. All three ports were coupled to capacitive sensors, and the two flanking ports, which served as the reward ports, were connected to a syringe pump system (NE-500, New Era Pump Systems, Inc., Farmingdale, NY) for Phidget-based boxes or a gravity-fed solenoid system for Arduino-based systems (see below), both of which were calibrated to automatically dispense water according to custom training protocols (MWorks). During habituation, the center port was sometimes connected to a syringe for manually dispensing water if the animal did not immediately engage with the center port. 

For Phidget-based boxes, the capacitive sensors (Phidget Touch Sensor 1129, Calgary, Alberta, Canada) were controlled by a USB microcontroller (Phidget Interface Kit 1018). The syringe pumps were connected via an RS232 adapter (Startech RS-232/422/485 Serial over IP Ethernet Device Server, Lockbourne, OH). Settings for the syringe port were tested for precise and consistent reward delivery of $~$0.02-0.06 mL per trial. For Arduino-based boxes, the capacitive sensors were controlled by a USB microcontroller (Arduino UNO or Arduino Pro Mini), running an I/O program (StandardFirmata) that interfaced with the main experiment program (MWorks 0.6, 0.7). A gravity-based water system coupled to two solenoid valves connected to a microcontroller (SparkFun COM-12959) delivered water rewards. Timing parameters for solenoid valve opening and closing were experimentally determined to deliver $~$0.02-0.06 mL per trial. 

To allow the experimenter visual access to the rats' behavior, each box was illuminated with red LEDs, not visible to the rats, and a USB webcam. 

\subsection{Visual Stimulation}
% Include github refs to povray and morph generation
All stimuli were presented on a monitor (Dell P190S,
Round Rock, TX; Samsung 943-BT, Seoul, South Korea) positioned $\sim$30 $cm$ from the animal. Stimuli were reproduced following previous studies\cite{Zoccolan2009}. Visual objects were renderings of three-dimensional models built using a ray tracer package POV-Ray (http://www.povray.org/). Each object was defined as a particular configuration and blend of spheres. The particular objects selected for the anchors were modeled to replicate the stimuli used in a previous study\cite{Zoccolan2009}. Figure\ref{fig:basic_training} shows the ``default'' object views used during phase 1 of training. Objects were rendered with the same light source location and matched to have approximately equal height, width, and area, as defined by the a bounding box surround each object rendering. Object transformations (\textit{e.g.}, size, in-depth rotation) were generated using custom Python wrappers and the POV-Ray API). Morph stimuli were generated by gradually adjusting the relative proportions of each object, by parametrically shifting the spheres defining one object into the spheres defining the other. We used the Euclidean distance in pixel space to quantify the difference between each neighboring pair of images. 

\subsection{Task Acquisition}
The basic task was designed according a previous study using a similar paradigm\cite{Zoccolan2009}. The goal was to train the animals to initiate a trial by licking a center contact sensor (feeding tube wired to a capacitive sensor), which triggered the appearance of 1 of the 2 target visual objects on the screen. Animals then had to indicate which of the two objects was present by licking the left or the reward contact sensors (and reward ports) for object 1 or object 2. 

\subsubsection{Handling and Habituation}
Long-Evans rats (Charles River Laboratories, Wilmington, MA) of about 250 g were allowed to acclimate to the colony environment for about a week after arriving. Due to daily handling (as opposed to home-cage training), rat were habituated to human interaction for 1-2 days. Rats were accustomed to the experimenter by short sessions of picking up and petting the animals, until they exhibited signs of being acclimated, such as grooming during handling or accepting treats directly from the experimenter. After habituation to handling, rats were introduced to the training cages and acoustic signals generated by the behavior rigs. Water-deprived rats were encouraged to poke their heads out of the port hole (see ``Training Apparatus'') by manually offering them water with syringes connected to a feeding needle identical to the ones used in the behavior rigs. Fully acclimated rats were then transferred to the behavior rigs for regular training sessions.

\subsubsection{Training and Testing}
\paragraph{Phase 0: Always Reward}
On the first 1-2 days in the training boxes, a glob of peanut butter of Nutella was placed on each of the 3 lick ports to entice the animals to engage with the lick ports. The reward ports were triggered to dispense a small reward anytime the animal licked the feeding tubes. Trials proceeded in accordance with lick port detection, but negative feedback was withheld. If the rat continued to lick the correct reward port, the additional reward amount was delivered, as in standard training. This period typically lasted 1 session or less, as animals readily licked the reward ports after several presentations of the water reward.

\paragraph{Phase 1: Default View}
Animals were trained on a single view for object A and a single view for object B. This default training view was a large (\ang{40} of visual angle), front-facing view (\ang{0} in-depth rotation) of the object (as shown in Figure\ref{fig:basic_training}A). To acquire an award, rats had to provide correct responses (no more automatic reward) according to their object-port mappings, whether the left port corresponded to object A or object B, and vice versa for the right reward port. Negative feedback was also started during this phase, and continued throughout the remaining phases (except for ``No Feedback'' trials, see `Phase5b' below). 

Upon triggering a trial, one of the two objects appeared on the screen. Animals then had 350 ms to 3.5 s to indicate whether object A or object B was on the screen, by licking one o the two flanking reward ports. If animals attempted to response before the 350ms from stimulus onset, the trial was aborted. This was to prevent the animal from spurious, uncontrolled licking. If correct, the animal was given a water reward, and the stimulus was left on the screen for an additional 4 s while the animal retrieved the reward. If incorrect, no reward was delivered and a 1-3 time-out sequence began (failure tone with the monitor flickering black to middle gray at 5 Hz), during which time no new trials could be initiated. 

In addition, several measures were implemented to prevent bias formation. First, to address response bias, the training protocol tracked whether animals licked the same reward port too many times incorrectly, that is, continuing to lick the same reward port independent of the stimulus shown. Second, to prevent response bias due to stimulus presentation, a hard limit was set on the number of times the same stimulus could appear in a row (N=5 back-to-back presentations of the same stimulus). If either of these biases were flagged, the other stimulus was shown until the animal correctly responded, at which point the bias counts restarted from 0. Rats usually took about 3-10 days to achieve criterion performance (70\%).  

\paragraph{Phase 2-4: Training Views}
After the first session in which rats reached criterion on the single views of object A and B, limited additional views of each object were presented. In a given session, either changes in size or changes in depth-rotation were introduced, but not both (Phases 2 and 3). A staircase procedure was used to slowly introduce increasingly different views (difference from \ang{0} rotation or difference from \ang{40} size). Rats took about 5-10 days total (1-3 days for size, 1-4 days for rotation, of which there were more levels than size) for the staircase to reach the most extreme views for each transformation axis while maintaining criterion levels of 70\% overall accuracy. After each transformation axis was tested separately, rats that maintained criterion performance were tested for one session on combinations of the training views (Phase 4, the cross outlined in Figure\ref{fig:behavior_generalization}E). 

\paragraph{Phase 5a:  Generalization to Identity-Changing Transformations}
Rats that passed criterion levels of performance on the training views were then tested in subsequent sessions on novel, untrained views. To test how rats' perceptual choices mapped onto the parametrically-varying morph axis between the two original objects, a morph image was presented on interleaved trials for a small proportion of the session (<15\% of trials). Morphs were presented along the default view (size \ang{0} of visual angle, \ang{0} in-depth rotation). No feedback was provided on morph trials.

Morph probe trials were presented across several sessions to acquire sufficient trial numbers to fit psychometric curves. For each animal, responses to morph trials were then fit with a logistic function via  maximum likelihood estimation\cite{Virtanen2020} using a Python implementation of psignifit\cite{Schutt2016, Wichmann2001a}.

\paragraph{Phase 5b:  Generalization to Identity-Preserving Transformations}
Rat were tested on identity-preserving transformations with different combinations of size and in-depth rotation. A subset of views were reserved as ``No Feedback'' conditions in order to assess the extent to which generalization occurred spontaneously (blue boxes in Figure \ref{fig:behavior_generalization}E). With feedback, animals could learn each stimulus-response mapping for all tested views, whereas true generalization refers to recognition of the object in novel, untrained views (as in real-world scenarios). Different animals were assigned different No Feedback conditions as the test views. There could be differences in performance for feedback or no-feedback depending on which views are assigned as the test views, for example, poor performance on test views could be due to poor generalization, or more difficult views, such as smaller size. To control for this, an additional subset of views that were matched in size (acuity-matched views) were assigned to each corresponding set of test views (orange boxes in Figure \ref{fig:behavior_generalization}E).
 
% ----------------------------------------------------
% Surgery
% ----------------------------------------------------
\section{Surgical Procedures}
\subsubsection{Headplate implantation}
Aseptic surgical technique was followed during all survival surgeries. A headplate and cranial window were implanted in the same surgery as viral injections using methods modified from mouse cranial window procedures \cite{Goldey2014}. Rats were administered dexamethasone (2 mg/kg) $\sim$3 hours prior to surgery in order to reduce brain swelling. Rats were anesthetized using isofluorane in 100\% O2 (induction, 3-5\%; maintenance, 1.5-2\%), and placed in a stereotaxic apparatus (Knopf Instruments, Angle Two, Leica). Eyes were protected from drying out ophthalmic ointment (Puralube), and then covered with surgical drape to protect from direct light. Heart rate, breathing rate, oxygen saturation, and body temperature were measured with a pulse oximeter and commercially available software (PulseOx, Mouseox). Body temperature was maintained at 38$\circ$C with a feedback-controlled heating pad. 

The top of the head was shaved above the incision site, followed by an application of Nair to clear the site of any hair prior to incision. The exposed scalp was cleaned with saline, then wiped with three rounds of Povidone-Iodine swabs (Medline). A small lidocaine block (<0.5 cc) was administered along the incision site, which spanned from just behind the ears to the back of the head. After the incision, the skull surface was thoroughly cleaned with hydrogen peroxide (Swan) and a mixture of citric acid (10\%) and ferric chloride (3\%) (Parkell S393). 


Prior to head plate attachment, the center of the craniotomy was marked at -7.0 to -8.5 mm AP, 4.5 to 6.5 mm ML, depending on the areas being targeted for each animal. The custom titanium head plate was attached to the skull over the right hemisphere using several dental glues, and an optimized series of steps for strong adhesion that prevented rats from ripping off the implants. The implant procedure did not require any bone screws or additional supplements to keep the implant stable across months. First, a series of small indentations were placed using a small drill all across the cleaned skull to increase surface area and texturize the skull in preparation for adhesives. Prior to applying any adhesives, an extremely thorough cleaning was done of the bone surface with 2-3 rounds of hydrogen peroxide and saline washes, followed by 1 round of citric acid (Dentin Activator, Parkell S393) for 30 sec that was then thoroughly rinsed with sterile saline). The skull surface was completely dried off with highly absorbent, sterile eye-spears (Medline). The flesh around the wound margin was sealed with Vetbond (3M) to ensure no moisture would leak into or under any glued areas. The first layer of adhesive was a thin layer of dental glue, evenly applied in one layer across the exposed skull (Quick Base, S398, Catalyst, S371, and Powder, S396, part of C\&B Metabond kit, Parkell S380, all mixed on top of an ice tray). The headplate was placed at 40 degrees relative to Bregma, which matched the orientation of the imaging plane and captured most of the targeted areas of visual cortex. An adaptor mounted to the stereotax frame held the headplate in place, while the dental glue was carefully applied, being careful to leave the craniotomy site clear. This initial gluing was followed by a bulk gluing of a thicker dental glue (Dentsply Integrity Caulk) that provided structural filling and additional support for the angled headplate. The last step of the implant was to fill all remaining gaps with the C\&B Metabond dental glue. 

\subsubsection{Cranial window}
After the head plate was securely glued to the skull surface, the animal was re-affixed to the stereotax frame with the headplate. Since the headplate was attached at a steep angle, drilling the craniotomy and the remainder of the window surgery could be done on a ``flat'' surface by affixing the animal by the headplate. A 4-5mm diameter craniotomy was performed at the marked site by careful thinning of the bone with a dental drill within the circular area (Aseptico). Care was taken throughout the drilling process to keep the thinned region within the circular boundaries using a pair of surgical calipers (Fine Surgical Tools). Skull thinning was complete once the entire circular region was semi-transparent and blood vessels were clearly visible through the thinned skull. Once the skull was thinned down, the region was kept immersed in sterile saline for the remainder of the surgery. The remaining thinned bone was removed with laminectomy forceps (Fine Science Tools). The dura was cut open using a beveled 36G needle tip that was bent such that the fine tip curved upward. This was effective for gently lifting up the dura enough to create a small incision point, without risking pressure or puncture on the cortical surface beneath the dura. Flaps of dura were then peeled back with fine forceps or spring scissors to expose the brain surface, and tucked away around the edges of the craniotomy. Intracortical injections were performed after the duratomy while the entire area was submerged in sterile saline (see Viral Injections).

A window composed of stacked glass coverslips (four to five 4mm, plus one 5mm, Warner Instruments) bound with optical adhesive (Norland No.71) was then placed over the brain surface. The remaining saline was partially absorbed out with sterile eye-spears, and the craniotomy was sealed with cyanoacrylate glue (Vetbond, 3M) over a thin layer of sterile saline. Post-operative animals were administered buprenorphine (0.01-0.05 mg/kg) and carpofen (5 mg/kg) daily for 5-7 days following the surgery.

\subsubsection{Viral injections}
Intracortical injections were made at multiple sites ($\sim$5-9 sites per cranial window, spaced 0.5-1mm apart) using a microinjector (NanoFil, World Precision Instruments) fit with a 36G beveled needle (NF36BV-2, WPI). A total of $\sim$500-750nl was injected per site at a constant rate of 10-25nl/min at a depth of 750$um$ below the surface. A high-titre solution of viral vector (AAV9-syn-jGCaMP7f-WPRE) was diluted to a final ratio of 2:1 with a 20\% mannitol solution (Sigma-Aldrich) to promote diffusion. pGP-AAV-syn-jGCaMP7f-WPRE was a gift from Douglas Kim \& GENIE Project (Addgene viral prep \#104488-AAV9). Trace amounts of Fast-Green (Sigma-Aldrich) were added for visual confirmation of injected solution in the brain (see Figure\ref{fig:surgery_steps}). The exposed brain surface remained submerged in sterile saline throughout the injections.

% ----------------------------------------------------
% Area identification
% ----------------------------------------------------
\section{Wide-Field Mapping}
\subsection{Animal preparation}
About 20 minutes prior to the mapping session, animals were anesthetized with isofluorane (5\% induction, 1-1.5\% maintenance) and administered a subcutaneous dose of cholorprothixene (2 mg/kg, Sigma-Aldrich). During the mapping session, animals remained lightly anesthetized with minimal isofluorane (0.5-1\%), and a small dose of choloprothixene (2mg/kg). Anesthesia levels were tested with the paw-pinch reflex and breathing rate. The left eye facing the monitor was checked between trials to ensure it remained open and stable. 

\subsection{Tandem-lens macroscope}
Wide-field mapping was done with a tiltable, tandem-lens macroscope\cite{Ratzlaff1991, Kalatsky2003}, composed of a USB 3.0 CCD camera (MantaG033-B, Allied Vision) and 2 Nikon lenses (Nikon, 105-mm and 55-mm). Images were acquired at 25 Hz with 3x3 pixel binning (256x492 pixel ½-type sensor). Epifluorescence illumination was achieved with a 470 nm LED (Thorlabs) that was filtered and reflected through a filter cube that housed an excitation filter, dichroic mirror, and emission filter (Thorlabs). Green fluorescence or reflected light was collected and passed through the filter cube then focused on a CCD detector. For epifluorescence illumination, we used a 470 nm LED filtered and reflected by a long-pass dichroic mirror, and emitted fluorescence was filtered and captured at an imaging rate of 25Hz using custom Python scripts.

% Compare w. Wekselblatt et al. 2016:  Camera lenses allow a relatively high numerical aperture (NA) for light collection, which can also be adjusted easily using the f-stop setting to restrict the NA. This permits a flexible trade-off between sensitivity and depth of field, especially as increased depth of field is useful, given the curvature of the cortical surface. Imaging was generally performed at an f-stop of 5.6. The ratio of the focal lengths of the two lenses determines image magnification. To map 1 cm of cortex across the 2-cm detector (6.5 μm pixels), we chose 50 mm and 105 mm lenses, yielding magnification of 2.1× and 3.1 μm specimen pixels. In practice, we find an effective spatial resolution of ∼25 μm, based on the highest spatial frequencies present in nonbinned images of vascular structure. Binning across spatially oversampled pixels can reduce shot noise by allowing more total photons to be detected with increased illumination or NA. This is a standard practice in intrinsic signal imaging (Kalatsky and Stryker 2003) and is generally applicable at high light levels, where readout noise is negligible compared with photon count noise.

\subsection{Visual stimulation}
Visual stimuli were presented using custom Python scripts on a 72 inch LCD monitor (LG). The monitor was centered in front of the left eye, spanning 177 degrees of visual field along azimuth, 67 degrees along elevation. A periodic stimulus consisting of a bar sweeping across the screen \cite{Kalatsky2003, Marshel2011} was presented to the (left) eye contralateral to the cranial window. The bar subtended 5 degrees of visual angle, and was either presented as a white bar sweeping over a black background or an apertured bar, containing one of 32 possible natural scene images, drifting over a gray background. The bar was presented at 0.13 Hz along the azimuth and elevation axes, for a total of 2 (downward, rightward) or 4 (downward, rightward, leftward, upward) conditions. The selected stimulation frequency was one of a subset tested that avoided frequency ranges of known, non-stimulus-driven, physiological signals (\textit{e.g.}, heart rate or breathing rate). One trial consisted of 10 cycles of the bar, and a total of 4-5 such trials were acquired for each direction. To preserve the speed of the bar between azimuth and elevation conditions, the bar traversed the full extent of the monitor's width centered along the monitor's vertical extend (bar started and ended off screen). 

\subsection{Image processing}
Raw fluorescence signals were corrected for slow drift by removing the rolling average of each pixel’s time course. The width of the rolling window was set to 2.5 times the length of the stimulation period to remove slow linear and non-linear trends. For each pixel, the time courses for each trial (10 cycles of the stimulus moving along a given direction) were aligned and averaged for each condition (1 of 4 possible directions). We then performed a Fourier spectral analysis on the averaged time series for each pixel to get a magnitude and phase value for each pixel at each frequency. The strength of the response to the stimulus was calculated as the ratio of the Fourier magnitude at the frequency of stimulation to the sum of the magnitudes at all other frequencies\cite{Kalatsky2003, Marshel2011, Juavinett2017}.

\subsection{Area segementation}
Retinotopic maps were created by taking the phase values for all pixels in the image and converting them to Cartesian coordinates that matched the linear position of the bar on the monitor to the phase of the stimulus cycle that corresponded to that position. To identify the borders between visual areas, maps of vertical and horizontal retinotopy were combined to calculated a visual field sign map\cite{Juavinett2017, Zhuang2017}. The visual field sign was computed by taking the sine of the difference between the vertical and horizontal retinotopic gradients at each pixel. Sign map were then filtered and thresholded to reveal key visual areas:  areas with mirror representations map to 1 (blue) and areas with nonmirror representations map to -1 (red), as shown in Figure\ref{fig:retino_mapping}. 
While multi-site viral injections allowed for greater coverage of exposed cortex, they also resulted in patchy expression levels in a subset of animals. Animals with ambiguous retinotopic maps were excluded from further study. We followed previous systematizations for area identification\cite{Juavinett2017} and used a combination of relative location to the other identified areas, relative cortical area, and visual field sign. For visualization, phase and power maps were smoothed with a Gaussian window (FWHM=$50um$) and masked by applying a threshold to the magnitude ratio.  

% ----------------------------------------------------
% 2p imaging
% ----------------------------------------------------
\section{Two-photon calcium imaging}
A battery of stimuli were used to characterize responses across rat lateral visual cortex. Briefly, a moving bar stimulus was used to map two-photon retinotopic preferences and register two-photon fields-of-view to wide-field maps identified with the same stimulus paradigm. To measure more fine-scaled receptive field properties, a standard tiling stimulus was used to estimate the position, extent, and shape of receptive fields. To estimate responses to simple features such as edge orientation or direction tuning, a set of drifting gratings were used. Finally, to characterize more complex object representation, as tested in the behavioral choices of trained rats, a subset of the same object stimuli were used to measure neuronal responses in naive rats. While localizer runs only used the moving bar and receptive field protocols, subsequent functional runs tested the full suite of stimuli.  

\subsection{Visual stimulation}
Head-fixed animals passively viewed visual stimuli presented on an LCD monitor (LG, 1920x1080 resolution, 60 Hz refresh rate), subtending \ang{119} of visual angle along azimuth and \ang{67} degrees along elevation axes of the visual field contralateral to the cranial window (see Figure\ref{fig:2p_retino}). Stimulus presentation was synced to neural data acquisition for each trial using custom software (MWorks, Python). 

\paragraph{Moving bar.} A white bar subtending \ang{2} or \ang{5} degrees of visual angle cycled across a black screen at a stimulation frequency of 0.24 Hz or 0.13 Hz (sufficient maps were acquired with either stimulus combination). Four cardinal directions were tested (downward, upward, leftward, rightward). Each ``trial'' consisted of of 12 cycles of the stimulus, and 4-6 trials were presented for each of the four conditions, totalling 16-24 trials total. Blank trials were also included to measure baseline fluctuations in spontaneous activity.  

\paragraph{Receptive field mapping.}
To estimate receptive field size and positions, we adapted a standard mapping protocol~\cite{Marques2018}. The monitor was subdivided into square tiles, in which a square-wave, drifting grating was presented at a spatial frequency of 0.25 cycles/deg, and driting speed of 10 cycles/sec. On each trial, the drifting grating randomly switched every 125 msec between 4 cardinal directions. Each tiled position was stimulated for 500 msec, followed by 1 sec inter-trial interval (ITI). Since the receptive field mapping used a shorter ITI, we restricted the stimulated position of a given trial to be a minimum of 30 degrees of visual angle away from the position of the previous trial. 

The size of the square aperture, \textit{i.e.}, the tile, was either \ang{5} of visual angle or \ang{10} of visual angle. For the small tile (5 degrees), this resulted in 21 positions along azimuth and 11 positions along elevation, totalling 231 positions. Each position was stimulated a minimum of 10 times total across 5 blocks, with 2 repetitions of each of 231 positions per block. For the larger tile (10 degrees), this resulted in 11 azimuth positions and 6 elevation positions, totalling 66 positions. Each position was stimulated a minimum of 10-20 times total across 2-4 blocks, with 5 repetitions of each of 66 positions per block. Since the larger tile required significantly fewer conditions than the smaller tile, it was used for most localizer runs with more reptitions (20, instead of 10) per position. 

\paragraph{Drifting gratings.}
To measure visual feature tuning, we presented drifting, square-wave gratings at 8 directions (ang{0} to \ang{315}, steps of \ang{45}). Gratings were presented at either full screen or within a circular aperture whose size was determined by the average receptive field size of the population recorded in previous localizer sessions. All gratings were also presented at two spatial frequencies (0.5 and 0.1 Hz) to target the low and high end of known visual acuity levels~\cite{Prusky2000}, and at two speeds (10 deg/s and 20 deg/s). This set of stimulus configurations resulted in 64 unique grating stimuli, which were repeated $\sim$20 times in pseuedo-random order across 4 blocks, such that each block contained 5 repetitions of each of the 64 conditions. Gratings were presented on a gray background (luminance-matched) for 500 msec, followed by 2 sec inter-trial intervals of blank gray screens. 

\paragraph{Objects.}
For two-photon imaging experiments, a subset of the object stimuli tested on the trained rats were used (see ``Behavior Training, Visual Stimulation''). Each morph (7 intermediate morphs, plus the 2 anchors) was presented at 5 different sizes (10-\ang{50} of visual angle, in \ang{10} steps). For each size, mean luminance was measured with a photometer placed at the level of the rat's eye to determine the gray-scale values needed to create full-screen stimuli matched in luminance for each size and object. Luminance differences between morphs were small relative to changes in size (see Figure\ref{supfig:stimulus_metrics}), so 1 luminance control was assigned for each of the object sizes tested. This resulted in 50 unique conditions, each presented a minimum of 30 times across 6 blocks, with 5 repetitions of each of the 50 condition per block. Stimuli were presented for 1 sec, followed by a 2 sec inter-trial interval. 


\subsection{Neural Data Acquisition}
Neural imaging data was collected using a custom-built galvo-resonant scanning two-photon microscope (20 kHz; Cambridge Technologies) and a 0.8 WD/16x water-immersion objective (Nikon, CF 175). A mode-locked Ti:Sapphire laser (80 Mhz, MaiTai-eHP DeepSee, pre-chirped, Spectra-Physics) 
provided 920 nm excitation for both channels. Emission was collected using green and red filters simultaneously on two photomultiplier tubes (Hamamatsu, H10770PA-40).
% Emission was collected using green (535/50 nm) and red (610/75 nm) filters (\ref{REFREF_2p_filters} Chroma) simultaneously on two photomultiplier tubes (Hamamatsu, H10770PA-40).

Laser power was controlled by an electro-optic modulator (Pockels cell, Model 350-80-LA, ConOptics, Inc.). Power measured at the object ranged 30-80mW, likely an overestimate of actual power reaching the tissue through the cranial window. The beam was first expanded 3x with a telescope configuration of a 50- and a 150-mm (Thorlabs). The expanded beam could then proceed through one of two paths. The first led to a standard, high-resolution imaging mode (minimum 500 x 500 $um$) in which the spot size was expanded an additional $~$4x to fill the back aperture of the objective. The second path allowed for a larger FOV (minimum 1x1$mm$), with a $~$2x beam expansion that slightly under=filled the back aperture. 

%\ref{REFREF_scopelens} 
 
The microscope body was tilted at an angle of 40 degrees to match the plane of the cranial window. For functional datasets, single plane images were collected at a rate of 44.65 Hz (512x512 pixels; 1$mm$ x 1$mm$ FOV) using ScanImage\cite{Pologruto2003} (ScanImage 2016, Vidrio Technologies). 

For anatomical volumes, a 200$um$ z-stack was taken in steps of 20$um$ simultaneously for both channels. A total of 100 volumes were taken and averaged for anatomical images. Depth was controlled with a Piezo controller/amplifier (Physik Instrumente, PI E-665-CR). For localizer and functional runs (see below), the FOV was placed 150-300 um below the bottom layer of cranial window or the surface blood vessels. 

Light shielding around the objective was used to block light emitted from the LCD monitor. A stack of O-rings (McMaster-Carr, Buna-N O rings 016-018) glued over the headplate created a dish-like receptacle above the cranial window\cite{Goldey2014}, and mated with another O-ring attached to a black shroud around the objective. This created both a light- and water-tight seal for the water immersion objection during visual stimulation. 
%%

\subsubsection{Localizer Sessions}
A set of target candidate FOVs were identified in each animal with a viable window (clear cellular resolution, identified visual areas). Given the large suite of stimuli included in the survey and to maximize repeitions per condition, localizer sessions were run for each FOV to validate that the target FOV was in the visual area of interest, and to identify the best stimulus location to present localized, or non-full screen, stimuli for the objects and gratings blocks. For a given FOV, the targeted stimulus location for functional sessions was determined by taking the center of mass across all cells' receptive fields. As an additional constraint, only FOVs for which the bounding box around the largest sized stimuli were within the monitor's bounds were accepted for functional sessions.

Two-photon FOVs were registered to wide-field vasculature maps offline by aligning blood vessels present in both images. Matching points between the two views were manually selected based on uniquely identifiable junctures, and a transformation matrix warped one image onto the other. 

\subsubsection{Functional Sessions}
Since the kinematic mounts allowed the same cells to be identified across days (see Figure\ref{fig:multiday_imaging}), it was possible to return to the same FOV that was mapped in a previous localizer session (usually mapped the previous day). Localized stimuli (drifting gratings and objects) were then centered at the target stimulus coordinates identified in the localizer sessions. In addition, both the moving bar retinotopic mapping paradigm and the receptive field mapping paradigm, both used in the initial localizer session, were repeated in the functional sessions. This allowed us to acquire receptive field estimates in the same cells and the same sessions in which object and gratings responses were collected, while avoiding the concern that different cells might be active during functional sessions than localizer sessions, or that receptive fields might subtly change from one day to the next.

\subsection{Behavior Acquisition}
High-resolution images of the animal's behavior state were acquired in sync with neural data acquisition using custom Python software. A CCD camera (Manta G-033, Allied Vision, SonyICX414 sensor) with a zoom lens was centered on the animal's head and upper body on the side facing the monitor, illuminated with an IR LED. Images (492x656, 9.9 x 9.9 $um$ pixels) were acquired at 20Hz.

\subsection{Image Processing}
Motion correction, ROI selection, neuropil correction, and trace extraction were done with a custom pipeline written in Matlab and Python. Motion correction used rigid transformations within each FOV using custom Matlab code (Chris Harvey, Harvard Medical School). 

\subsubsection{Cell Mask Identification}
For ROI selection, an activity map was created by taking the standard deviation across motion-corrected frames within a movie file for each block of trials, and then taking the maximum projection across all blocks. ROIs were selected manually with a circular mask using a custom Matlab GUI. To remove background calcium signals, we estimated neuropil masks as circular annuli of 11$um$ width, with the inner edge at 9$um$ beyond the outermost edge of a corresponding cell body and the outer edge extending to 20$um$. Pixels from adjacent cell body masks were excluded from the neuropil masks. 
\subsubsection{Time Course Extraction and Correction}
To get raw fluorescent traces for a given mask, the fluorescence intensity of a cell at each time point was computed as the average fluorescence across pixels within the mask. To correct for slow drift effects due to long imaging sessions, a correction procedure was applied. First, a baseline $F_0$ signal was extracted with a sliding filter (20\% percentile of a 30 sec sliding window) for each cell in each movie. For each trace, this drift was subtracted from the raw trace before adding back the mean of the offset value. 
To account for neuropil signals that could contaminate the soma trace, neuropil correction was applied as follows: $F_{soma}$(t)=($F_{raw}$(t) - $F_{neuropil}$(t)) + $\overline{F}_{neuropil}$, where $t$ is time, $r$ is an empirical constant defined as the ratio $F_{bloodvessel}$/$F_{neuropil}$ (set to 0.7), and $\overline{F}_{neuropil}$ is the temporally-averaged mean of the neuropil fluorescence\cite{Liang2018, Kerlin2010}. For a subset of data, both Suite2p and CaImAn were tested.

Fractional change in fluorescence, $\Delta$F/F(t), following visual stimulus presentation was calculated as: $\Delta$F/F(t)=(F(t)-F$_0$)/F$_0$, where F(t) is the corrected fluorescence trace during the stimulus presentation flanked by 1 sec before and after stimulus onset and offset, and F$_0$ is the 1 sec baseline period prior to stimulus onset. Single response values for a given trial were obtained by averaging the $\Delta$F/F(t) response during the stimulus presentation window x 2. The additional factor was to account for cells that exhibited OFF-type responses or slow response kinetics. 

\subsection{Area Identification and Validation}
Each two-photon FOV was coregistered to wide-field retinotopic maps using blood vessel markers. All two-photon imaging sessions began with the acquisition of an anatomical volume, which was a  500$um$ z-stack taken from the surface. Prior to the start of the imaging session, rats were given subcutaneous injections of SR101 (Sigma-Aldrich, S7635) for fluorescent labeling of the blood vessels visible in the red channel. 

Two-photon blood vessel images were matched to wide-field maps offline. Matching points between the two images were selected based on uniquely identifiable blood vessels, then used to identify a transformation matrix to warp one image into the other. Assignment of two-photon FOVs were validated based on retinotopic maps measured with the same cycling bar paradigm used for wide-field maps of azimuth and elevation. Sign maps were obtained from the retinotopic maps with a series of morphological filters\cite{Marshel2011, Garrett2014, Zhuang2017}, which were then used to identify patches representing putative visual areas. Two-photon FOVs were segmented based on the visual field sign maps (see Wide-Field Mapping, Area Segmentation). Since a given FOV could contain more than one visual area, cells were assigned based on both the segmented two-photon sign maps and the wide-field sign maps. Only two-photon FOVs with matches to wide-field maps and unambiguous sign reversals were included for subsequent analyses. 

\subsection{Pose Extraction}
Image frames were converted to mp4 videos to be analyzed using DeepLabCut v2.0\cite{Insafutdinov2016, Mathis2018, Nath2019}. A total of 16 different videos (20 frames per video, automated K-means clustering), sampled from 12 different imaging sessions and each of the stimulus experiment types, were used to train the pre-trained network (ResNet50, training size 0.95, batch size 8).

Points defining the eye, snout, and paw on the side facing the monitor were tracked in the following way. For the eye, 4 points (top, bottom, left corner, right corner) were selected around the eyelid for the general outline of the eye, and within this region, 5 additional points were selected: 4 points specifying the pupil shape, which were later fit with an ellipse, and an additional 5th point corresponding to the corneal reflection, CR. For the snout, the 3 most posterior whiskers on the whisker pad were each assigned 3 adjacent points along the line of each whisker. In addition, the tip of the noise and two adjacent points along the top of the snout (toward the eyes), and two adjacaent points along the bottom (towadr the mouth), were selected. The rat's front-left and front-right paws were also included for tracking, as they were visible during periods of grooming, and could thus be used to filter out trials in which the animal was not passively viewing the stimuli. 


% ----------------------------------------------------------
% Retintopy/Receptive fields
% ----------------------------------------------------------
\subsection{Estimation of retinotopic tuning preferences}
\subsubsection{Retinotopic preferences of background neuropil}
The center of each neuropil ring was first assigned a value corresponding to the preferred retinotopic location of the neuropil ring (average over all pixels within the ring). The center was then dilated to a disk of 20$um$ radius, averaging the preferred retinotopy for overlapping disks. Pixel-wise estimates of retinotopic preference were obtained by smoothing the resulting dilated and averaged image with an isotropic two-dimensional Gaussian filter with standard deviation of 7$um$. 

For each FOV, the spatial axis corresponding to the direction of maximal retinotopic change for a given retinotopic axis (elevation or azimuth) was identified as follows\cite{Liang2018}. First, the two-dimensional pixel-wise gradient was calculated as:  
\begin{align}
\nabla Ret(x, y) = (\partial Ret(x, y)/\partial x)\hat{i} + (\partial Ret(x, y)/\partial y)\hat{j}.
\end{align}
The spatial axis was then computed as the normalized average gradient vector, $\overline{\nabla Ret}/ \|\overline{\nabla Ret}\|$, across all pixels. The smoothed neuropil map was then projected onto this mean gradient vector, such that for each pixel, its projected location along this new spatial axis was defined as:  $\tilde{x}=(x, y)\cdot \overline{\nabla Ret}/ \|\overline{\nabla Ret}\|$. The relationship between a pixel's preferred retinotopic location (based on the smoothed neuropil maps) and $\tilde{x}$ was modeled with a linear function, $R_{fit}=a\tilde{x}+b$, where $a$ is the fit parameter (in visual field degrees/cortical $um$) that corresponds to the rate of retinotopic progression along the map. The normalized mean gradient and linear fit were computed separately for azimuth and elevation.

\subsubsection{Estimation of receptive fields}
For each ROI, responses at each stimulated location were baseline subtracted (0.5 s before stimulus onset), then averaged across repetitions\cite{Marques2018}. An MxN stimulus response map was computed by averaging the responses during the stimulus window and 0.5 s after stimulus offset (1 s total). The response map $R(az, el)$, where $az$ and $el$ are the retinotopic coordinates in azimuth and elevation, respectively, was then fitted with a two-dimensional Gaussian curve, using the Python implementation of the least-squares Levenberg-Marquardt algorithm\cite{More1978, Virtanen2020}:
\begin{align}
\begin{split}
    R(az, el) = a +b*\exp\left[ -\left( \frac{(az-az_0)\cos\theta + (el-el_0)\sin\theta}{\sqrt{2} \sigma_1} \right)^2 \\ 
    -\left( \frac{-(az-az_0)\sin\theta+(el-el_0)\cos\theta}{\sqrt{2}\sigma_2} \right)^2 \right] 
\end{split}
\end{align}

where ($az_0$, $el_0$) is the center of the 2D Gaussian in azimuth and elevation, $\sigma_1$ and $\sigma_2$ are the standard deviations along the two axes of the Gaussian, $\theta$ is the angle of the Gaussian relative to the azimuth-elevation coordinate system, and $a$ and $b$ are offset and amplitude parameters, respectively. 

The receptive field boundary was considered to be the ellipse defined by the center ($az_0$, $el_0$) and standard deviations ($\sigma_1$, $\sigma_2$) of the fitted Gaussian:

\begin{align}
    \left[ \frac{(az-az_0)\cos\theta + (el-el_0)\sin\theta}{\sigma_1} \right]^2 + \left[ \frac{-(az-az_0)\sin\theta + (el-el_0)\cos\theta}{\sigma_2} \right]^2 = 1.
\end{align}

Only fits with $R^2$>0.5 and s.d. between \ang{2.5} and \ang{55} were included for further analyses. To determine whether the fitting procedure yieled a high-quality, reliable fit, we used a bootstrap procedure to estimate confidence intervals (95\% CI) for each estimated parameter. Specifically, for each of 1000 iterations, 10 trials per condition were sampled with replacement, then averaged and fitted according to the procedure described above. This generated a distribution of parameter estimates, which were then used to determine the 95\% CI for each cell's estimated RF parameters. Only cells with fits lying within the 95\% CI ($az_0$, $el_0$, $\theta$, $\sigma_1$, and $\sigma_2$) were included.

\subsubsection{Estimation of retinotopic scatter}
For each FOV, retinotopic scatter was estimated as the deviation, $D_{VF}$, in degrees of visual field space, from the predicted receptive field center based on its estimated cortical position:
\begin{align}
    D_{VF}=|Ret_{pref}(\tilde{x}) - Ret_{fit}(\tilde{x})|, 
\end{align}
where $Ret_{pref}(\tilde{x})$ denotes the cell's measured receptive field center, and $Ret_{fit}(\tilde{x})$ denotes its predicted retinotopic preference according to its projected location $\tilde{x}$ along the mean gradient axis. Cortical scatter, $D_{CX}$, in microns, was calculated as the absolute deviation from the predicted cortical position based on the spatial progression defined by the gradient axis:
\begin{align}
    D_{CX}=|\tilde{x}-(Ret_{VF}(\tilde{x})-b)/a|, 
\end{align}
which corresponds to the distance (along the spatial gradient axis) a given cell would have to move along azimuth or elevation in order to form a perfectly progressing smooth retinotopic map. 

\subsubsection{Spherical correction}
To correct distrotions in measured RFs due to monitor distance, standard approaches for spherical correcting stimuli were applied in reverse\cite{Labrigger2012}. First, monitor coordinates were mapped from Cartesian space to spherical coordinates from the rat's point of view, taking into account the distance, size of the monitor, and angle, relative to the rat's eye. Typically, the corrective distortion is applied to a visual stimulus to cancel out the distortion caused by the flat monitor covering a large range of visual angle: Based on the known measurements between the rat's eye and the monitor, a 3D model of the monitor can be created, and pixel locations of the monitor are mapped to the spherical coordinates of the monitor. X- and Y-coordinates are treated as angles of azimuth and elevation (see Figure\ref{supfig:spherical_correction_steps}A), and the distortion is applied using interpolation to map horizontal lines to isoelevation lines, and vertical lines to isoazimuth lines. If the stimuli are not corrected to cancel out the distortion, the otherwise ``distorted'' percept of the animal can be corrected by appling the corrective distortion directly on the receptive field map. To correct the RF maps, each map was upsampled to match the pixel coordinates of the monitor (resolution was downsampled by a factor of 3 for faster computations). Then, the upsampled map was mapped to spherical coordinates using the 3D model described above (monitor angle=0, distance=30 cm, monitor width, height, and center, relative to the eye). Finally, the warped image was trimmed back down to stimulus coordinates (degrees of visual angle, in \ang{5} of \ang{10} degree tiles), and the entire RF fitting process was performed. 

% ----------------------------------------------------------
% GRATINGS
% ----------------------------------------------------------
%\section{Estimation of tuning preferences}
\subsection{Tuning preferences of cells responsive to drifting gratings}
For each cell and each stimulus condition, we determined the evoked response to be significantly different from noise if the amplitude of dF/F during the stimulus period exceeded 2.5 standard deviations above or below the mean baseline activity (computed using 0.5s or 1s prior to stimulus onset) for at least 10 out of the 44 time points (44.65 Hz frame rate * (0.5 s stimulus presentation + 0.5 s after stimulus offset)).

To determine if a given cell showed a significant response at a particular stimulus configuration (a unique combination of size, speed, spatial frequency), we required that at least 2 out of the 8 directions at this given stimulus configuration evoked responses according to the above criteria. Axis selectivity, direction selectivity, and preferred theta were calculated following previously published metrics\cite{Liang2018}. 

\subsubsection{Direction tuning curve fitting}
For each cell that exhibited a significant response at a given combination of spatial frequency, size, and speed, a bootstrap analysis (1000 iterations of 20 samples each, to match the measured sample of gratings trials) was performed to compute direction tuning curves. Direction tuning curves were originally sampled in steps of \ang{45}. To obtain a more precise estimate of tuning parameters (preferred orientation and direction), tuning curves were fit with a two-peaked Gaussian\cite{Liang2018, Sun2016}:
\begin{align}
    R(\theta) = R_1 \exp{ - \frac{-(\theta-\theta_{pref})^2}{2\sigma^2} } + R_2 \exp{ - \frac{-(\theta-\theta_{pref}-\ang{180})^2}{2\sigma^2} } + R_{offset},
\end{align}
where $R(\theta)$ is the $\Delta$F/F(t) response for gratings direction $\theta$, $\theta_{pref}$ is the direction evoking the strongest $\Delta$F/F(t) response, $R_1$, $R_2$ is the amplitude of the second peak at $\theta_{pref} + \ang{180}$, and $R_{offset}$ is a constant amplitude offset. The model assumes the two peaks of the Gaussian are \ang{180} apart, and that the Gaussians have a common standard deviation, $\sigma$. Initial parameters were set as follows:  $\sigma$ was initialized as the step size (\ang{45}), $R_{offset}$ was the mean of responses to the null directions (all directions except for $\theta_{pref}$ and $\theta_{pref}+\ang{180}$). 

To improve the reliability of the fitting and the accuracy of estimated preferred direction and orientation, several additional steps were implemented by following a simplification of the procedure outlined in \cite{Liang2018}. First, we added a ninth point at \ang{360} by copying the point at \ang{0} to wrap the input values. Then, the number of input points was increased from 9 to 25 by linearly interpolating the 9-point tuning curve, to more finely sample the curve for the fitting procedure.  
A bootstrap procedure was used to fit the tuning curves. On each iteration, 20 trials were randomly sampled (with replacement) for each of the 64 conditions, then averaged, interpolated, and fit according to the steps outlined above. The final tuning curves for each cell (one for each unique combination of spatial frequency, size, and speed to which the cell was significantly responsive) were computed from the mean of the fitted parameters across the sampling iterations. 

To evaluate the quality of fits, a combination of criteria were used\cite{Liang2018}. For each iteration of the fitting procedure, a coefficient of determination, $r^2$, was calculated as the explained variance using least-squares regression to fit the data\cite{More1978, Virtanen2020}. A combined coefficient of determination, $r_{comb}^2$, was also calculated for the original tuning curve versus a fitted curve derived from the average of each fit parameter (across the 1000 iterations). These metrics were combined into goodness-of-fit heuristic, G$_{fit}$: 
\begin{align}
    G_{fit}=\overline{\sqrt{r^2}} (1-IQR_{\sqrt{r^2}}) \sqrt{r_{comb}^2},
\end{align}
where $IQR_{\sqrt{r^2}}$ is the interquartile range (difference between the $25^{th}$- and $75^{th}$-percentiles of $\sqrt{r^2}$ values across iterations. A cell was considered to have a well-fit tuning curve at a given combination of spatial frequency, size, and speed if its $G_{fit}$ was greater than or equal to 0.5. 

\subsubsection{Axis and direction selectivity}
For each cell exhibiting significant responses to gratings (see above), a vector sum axis selectivity index (ASI) was computed as a metric for selectivity of motion along a given axis\cite{Liang2018, Kerlin2010}. Axis selectivity is distinguished from orientation selectivity, as the latter is typically measured with static gratings, which the current study did not test. To calculate the ASI, the responses for each of the directions was projected onto a circle with $2i$ progression and the magnitude of the normalized vector sum was estimated according to: 
\begin{align}
    ASI=\big|\sum_{i=1}^{24} R(\theta_i) \exp^{2i*\theta_i}\big|/\sum_{i=1}^{24}|R(\theta_i)|, 
\end{align}
where ASI values ranged from 0 (no selectivity) to 1 (maximum selectivity). Opposite directions were additive, while orthogonal directions canceled each other out. The ASI was computed for 1000 iterations using the same bootstrap procedure as used in calculating the tuning curves. For each cell, the final ASI was computed as the mean ASI across all combinations of spatial frequency, size, and speed for which the cell exhibited significant evoked responses. 

A direction selective index (DSI) was computed in a similar way, the responses were projected onto a circle with $1i$ progression:  \begin{align}
    DSI=\big|\sum_{i=1}^{24} R(\theta_i) \exp^{1i*\theta_i}\big|/\sum_{i=1}^{24}|R(\theta_i)|.
\end{align}
DSI computations were iterated 1000 times, with the final DSI value for a cell taken as the mean DSI across stimulus combinations that evoked significant responses. 

A cell was determined to be direction-selective if a) it had a significant response to the grating stimuli (see ``Estimation of cells visually responsive for drifting gratings''), b) it had a well-fit direction tuning curve (see ``Direction tuning curve fitting''), and c) it had an average direction selectivity index (DSI) greater than or equal to 0.2\cite{Liang2018} for all stimulus combinations to which it had a significant response. Similarly, a cell was considered to be axis-selective if a) it had a significant response to the grating stimuli, b) it had a well-fit direction tuning curve, and c) its ASI exceeded 0.15 and its DSI was less than 0.2 for all stimulus combinations for which a significant response was observed.

\subsubsection{Estimation of preferred direction}
For direction-selective cells, preferred direction of motion was determined by taking the circular average of the fitted $\theta_{pref}$ across stimulus combinations for which a significant response was observed, and direction tuning curves passed goodness-of-fit thresholds. 

\subsubsection{Preference Metrics for Spatial Frequency, Size, and Speed}
For cells exhibiting significant visual responses to the drifting gratings, a preference index was calculated for each non-direction parameter, \textit{i.e.}, spatial frequency, size, and speed. Each non-direction parameter was tested at two levels, one low and one high, and the preference index were defined as:  $(R_{low}-R_{high})/(R_{low}+R_{high})$, which ranged from -1 (preference for lower) and 1 (preference for higher). For each cell, a bootstrap procedure was used to determine whether each preference index was significantly different from zero. Specifically, a preference index was calculated for each of N iterations by sampling trials (with replacement) for each of the 64 conditions, creating a distribution of preference index values. The cell's preference index was considered significant for that parameter if the 95\% CI did not contain 0 (no preference). To compare the distribution of preference indices across visual areas for a given parameter (spatial frequencty, size, or speed), only cells with a significant preference index for that parameter were included.

%%%% 
% ----------------------------------------------------------
% Objects
% ----------------------------------------------------------
\subsection{Tuning preferences of cells responsive to objects}

\subsubsection{Estimation of cells with significant visual responses to objects}
For each cell, stimulus evoked responses were determined to be significant using a receiver-operating-characteristic (ROC) analysis from signal detection theory\cite{Green1966, Britten1992, Busse2011}. For each cell, a distribution of stimulus responses and a distribution of baseline responses were obtained for each stimulus condition (one response value per trial). For each condition, an ROC curve was obtained across a range of criterion levels (50 levels, linearly sampled between the cell's minimum and maximum stimulus response values) by calculating the proportion of times the stimulus and baseline responses exceeded a given criterion level. The area under the ROC curve (AUC) was calculated for each condition, such that a value of 0.5 corresponds to fully overlapping stimulus and baseline response distributions, while increasingly larger values indicate increased separability between the stimulus and response distributions. The maximum AUC value for a given cell thus corresponded to the stimulus condition for which the cell's responses were maximally distinguishable from baseline responses. For each cell, the significance of its maximum AUC value was evaluated by a shuffle test:  for each of 1000 iterations, the baseline and stimulus labels were shuffled, ROC curves and AUC values were calculated, and the likelihood of the cell's maximum AUC was determined as the fraction of times the true AUC was greater than the AUCs derived from the shuffled labels. Only cells with significant AUCs (p<0.05) were included as visually responsive. 

\subsubsection{Single-cell metrics of selectivity and tolerance}
Neuronal selectivity to morphs was quantified by a morph tuning index\cite{Zoccolan2007}:
\begin{align}
MT=[n-(\sum_{i=1}^{9}R_i/R_{max})]/(n-1), 
\end{align}
where $R_i$ is a neuron’s response to the $i^{th}$ morph ($n=9$ morphs), $R_{max}$ is the maximum response amongst the morphs. As a measure of response sparseness, MT ranges from 0 (no shape selectivity) to 1 (maximally shape selective). Size tolerance was quantified by normalizing size tuning curves to their maximum values, then averaging those resulting values that were < 1:  $ST=<R_{test}/max(R_{test})>$, where $R_{test}$ is the mean response to a given stimulus size of a neuron’s most preferred object, and $<.>$ denotes the average across tested stimulus sizes where $R_{test}<max(R_{test})$\cite{Zoccolan2007}. 

\subsubsection{Luminance correlations}
Since changes in size come with unavoidable changes in luminance, we estimated the extent to which a cell's tuning for size could be explained by its tuning for broad luminance. For each cell, its size tuning curve was calculated at its reference morph, which was defined as the morph eliciting the cell's maximum response. The cell's luminance tuning curve was calculated as the cell's response to each of the size-matched luminance stimuli (fullscreen, grayscale images). The correlation coefficient between the cell's size tuning curve and luminance tuning curve was taken as a measure for how similar the cell's responses were for size and size-matched luminance levels. Cells were considered to be significantly luminance-modulated if their size and luminance tuning curves were significantly correlated or anti-correlated (Pearson's correlation coefficient, p<0.05).

\subsubsection{Single Neuron Discriminability}
Since the animals were naive, there was no a prior reason that a cell should prefer one object over the other. To quantify discriminability for cells that did exhibit a preference, selectivity for one anchor over the other was determined with a Mann-Whitney rank test. Only cells that were significantly selective (p<0.05)for one or the other object were included in the analysis.  

For object-selective cells, a receiver operating characteristic (ROC) analysis was used to determine the extent to which the two objects could be discriminated\cite{Green1966, Britten1992, Rust2010SelectivityIT}. Given two response distributions that arise from two different alternatives (image 1 or image 2), an ROC curve was generated by computing the proportion of trials for alternative 1 (``object A'') on which the response exceeded criterion versus the proportion of trials for alternative 2 (``object B'') on which the responses exceeded criterion, for a range of criterion levels. 50 levels were used that linearly spanned the range of the cell's minimum and maximum responses. The area under the ROC curve (AUC) was taken as a measure of discriminability between the two distributions, that is, how well the  neuron could discriminate object A from object B. 

\subsubsection{Calculation of Neurometric Cures}
For each object-selective cell (see ``Single Neuron Discriminability''), a neurometric curve was calculated to estimate how responses changed as the stimulus morphed away from the neuron's preferred object. Just as the trained rats needed to pass a criterion performance of 70\% (see ``Behavior Training'') before generalization tests, selective neurons were included for further analysis only if discriminability, taken as the AUC metric (see ``Single Neuron Discriminability''), was 70\% or higher.

For well-discriminating cells, if the preferred object was object A, the null object was set as object B, and vice versa. At each morph level, the two response distributions compared was the null distribution and the response distribution for the given morph level. At morph level 0, or the object that the cell did not prefer, the two distributions were created by randomly sampling (without replacement) half of the trials as alternative 1, and the remaining half of trials as alternative 2. For morph levels that moved away from 0, or the null object, and increasingly became more similar to the preferred object, the two distributions were the full null distribution and the morph distribution. 

ROC curves were generated by computing the proportion of trials on which the response exceeded criterion for alternative 1 versus the proportion of trials on which the response exceeded criterion for alternative 2, for a range of criterion levels that spanned the cell's minimum and maximum responses for the images being compared. The area under the ROC curve (AUC) was computed at each morph level, and the AUCs were then fit with a logistic function to via  maximum likelihood estimation\cite{Virtanen2020} using a Python implementation of psignifit\cite{Schutt2016, Wichmann2001a} to obtain a neurometric curve. Threshold values were bound to 0 and 1, and cells with curves that fell outside of this range were excluded from further analysis. 

\subsection{Arousal modulation}
% REFERF
Pupil area was taken as a readout for arousal state. Specifically, an ellipse was fit to each frame using the four pupil points (see ``Pose Extraction'') as the anchors for the major and minor axes. Only frames that passed the confidence estimates and for which an ellipse could be fit were included for analysis. Trials that included frames that did not pass these criteria were excluded. Since stimulus luminance causes changes in pupil size that are not directly related to arousal state, the following procedure was taken to identify high and low arousal trials. For each stimulus condition (50 conditions total for objects, see ``Visual Stimulation, Objects''), a pupil metric was calculated for each trial as the average fractional area during the stimulus period, where fractional area was defined as the area of the fit ellipse divided by the maximum area across the imaging session. All trials for a given condition were then sorted by pupil size, and the top and bottom third were taken as high and low arousal trials, respectively. Sample counts were equalized by random sampling (without replacement) across all stimulus conditions and pupil states. This procedure was repeated for each stimulus condition. 

%\subsection{Population readout}
\subsubsection{Population Discriminability}
To quantify discriminability, we trained linear classifiers (support vector machines) to discriminate the two original objects from the neural activity in each area. The linear-readout scheme is important in that it is a simple, biologically plausible processing step that amounts to a thresholded sum taken over weighted synapses. This classifier approach does not provide a measure for the total information present in the population, but rather estimates the lower bound on the information explicitly accessible to the population to support the visual task\cite{Hung2005, Rust2010SelectivityIT}.

Linear support vector machines (SVMs) were trained to discriminate object A from object B from neural responses. Each presentation of an image produced a population response vector \textbf{x} of size $Nx1$, such that repeated presentations form a cluster of points in $N$-dimensional space (see Figure\ref{fig:neural_generalization}). The linear readout amounts to finding a linear hyperplane that best separates the response clouds corresponding to each image from those corresponding to all other images. Specifically, the linear readout amounts to\cite{Rust2010SelectivityIT, Hung2005, Li2009}: 
\begin{align}
    f(\textbf{x})=\textbf{w}^T\textbf{x} + b, 
\end{align}
where \textbf{w} is a $Nx1$ vector of the linear weights applied to each of $N$ neurons (defines the orientation of the hyperplane), and $b$ is a scalar that offsets the hyperplane from the origin. To determine the population’s choice about which image was presented, a response vector \textbf{x} (population response to one image) was applied to the classifier, and negative values of f(\textbf{x}) indicate object A and positive values indicate object B. Performance was defined as the proportion of correct answers when asked to classify each image with a held-out test set never included in training. 

The hyperplane and threshold for each classifier was determined by a support vector machine (SVM) using the scikit-learn machine learning library (LinearSVC\cite{Pedregosa2011}). The data were split into train and test sets (20\%) after balancing numbers of samples per condition. We used a 5-fold cross-validation procedure on the train set to fit and evaluate each model:  of the trials partitioned for training, models were optimized where the best $C=\{10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 10^2, 10^3\}$ that yielded the highest accuracy was chosen. Performance was then assessed with the held-out test set. 

\subsubsection{Linear separability and generalization}
To test linear separability (see Figure\ref{fig:neural_generalization}), 80\% of the trials corresponding to object A and object B for each size were simultaneously combined to train and evaluate the models, while the remaining 20\% of trials was used to measure classifier performance. To test generalization, two regimes were used. In the first, ``Train one, test one'' (Figure\ref{fig:neural_generalization}B-D), each classifier was trained to classify object A and B at one of the 5 sizes, then tested on each of the remaining 4 untrained sizes. Each training set (for each size) included 80\% of the trials for a given size, while the test sets could contain either the remaining 20\% of trials of the same size (test accuracy on ``trained'' conditions) or 100\% of the trials at one of the other sizes (test accuracy on ``novel'' conditions). 

In the second regime, ``Train a subset, test one'' (Figure\ref{fig:neural_generalization}),  each classifier was trained with trials of  4 of the 5 sizes together, then tested with the remaining, untrained size. Each training set was composed of 80\% of trials at each of the 4 train sizes. The remaining 20\% of trials for each of the 4 train sizes combined to form the held-out test set (test accuracy on ``trained'' conditions), while 100\% of the trials of the remaining 5th size formed the other type of test (test accuracy on ``novel'' conditions). All combinations of 4 of 5 sizes were used to train different classifiers and test generalization performance on samples of the remaining fifth size.

\subsubsection{Population sampling}
In analyses in which a given metric, \textit{e.g.}, classifier accuracy, is presented as a function of the number of neurons in a pseudo-population, we applied a resampling procedure to measure the variability that can be attributed to the particular subpopulation of neurons or particular subset of trials used for training versus testing. On each iteration, we sampled a new subpopulation of neurons that were randomly selected (without replacement) from all cells aggregated across imaging sites and animals, for a given visual area, and trials were randomly assigned for training and testing (without replacement). Error bars were calculated as the SD of classifier performance across 100 iterations. Chance performance was computed by randomly assigning objects or images associated with each response vector and repeating the classification analysis.


\section{Quantification and statistical analysis}
For all pairwise tests, Wilcoxon signed-rank tests were used, unless otherwise specified. Significance values were set to p<0.05 or p<0.01. For all comparisons between visual areas, Mann-Whitney U-tests, with significance values corrected for multiple comparisons.

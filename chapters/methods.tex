\chapter{Methods}
\label{methods}

% Materials and methods
% ----------------------------------------------------
\section{Animals}
All experimental procedures were reviewed and approved by the Harvard Institutional Animal Care and Use Committee. Experiments were performed at Harvard University. Animals used in this study were female Long-Evans rats, 3 months or older, weighing 250-350g (Charles River Laboratories). Rats were housed on a ventilated rack under a 12 hour light:dark cycle with food and water ad libitum, except when water-restricted for behavior training. 

% ----------------------------------------------------
% Surgery
% ----------------------------------------------------
\section{Surgical Procedures}
\subsection{Headplate implantation}
Aseptic surgical technique was followed during all survival surgeries. A headplate and cranial window were implanted in the same surgery as viral injections using methods modified from mouse cranial window procedures \cite{Goldey2014}. Rats were administered dexamethasone (2 mg/kg) $\sim$3 hours prior to surgery in order to reduce brain swelling. Rats were anesthetized using isofluorane in 100\% O2 (induction, 3-5\%; maintenance, 1.5-2\%), and placed in a stereotaxic apparatus (Knopf Instruments, Angle Two, Leica). Eyes were protected from drying out ophthalmic ointment (Puralube), and then covered with surgical drape to protect from direct light. Heart rate, breathing rate, oxygen saturation, and body temperature were measured with a pulse oximeter and commercially available software (PulseOx, Mouseox). Body temperature was maintained at 38$\circ$C with a feedback-controlled heating pad. 

The top of the head was shaved above the incision site, followed by an application of Nair to clear the site of any hair prior to incision. The exposed scalp was cleaned with saline, then wiped with three rounds of Povidone-Iodine swabs (Medline). A small lidocaine block (<0.5 cc) was administered along the incision site, which spanned from just behind the ears to the back of the head. After the incision, the skull surface was thoroughly cleaned with hydrogen peroxide (Swan) and a mixture of citric acid (10\%) and ferric chloride (3\%) (Parkell S393). A series of small indentations were placed using a small drill all across the cleaned skull to increase surface area and texturize the skull in preparation for adhesives. 

Prior to head plate attachment, the center of the craniotomy was marked at -7.0 to -8.5 mm AP, 4.5 to 6.5 mm ML, depending on the areas being targeted for each animal. The implant procedure did not require any bone screws or additional supplements to keep the implant stable across months. A custom titanium head plate was attached to the skull over the right hemisphere using dental glue (C\&B Metabond, Parkell S380). The head plate was placed at 40 degrees relative to Bregma, which matched the orientation of the imaging plane and captured most of the targeted areas of visual cortex. 

\subsection{Cranial window}
After the head plate was securely glued to the skull surface, the wound margin was closed up, while leaving the circular region surrounding the craniotomy site exposed. A 4-5mm diameter craniotomy was performed at the marked site by careful thinning of the bone with a dental drill within the circular area (Aseptico). Care was taken throughout the drilling process to keep the thinned region within the circular boundaries using a pair of surgical calipers (Fine Surgical Tools). Skull thinning was complete once the entire circular region was semi-transparent and blood vessels were clearly visible through the thinned skull. Once the skull was thinned down, the region was kept immersed in sterile saline for the remainder of the surgery. The remaining thinned bone was removed with laminectomy forceps (Fine Science Tools). The dura was cut open using a beveled 36G needle tip that was bent such that the fine tip curved upward. This was effective for gently lifting up the dura enough to create a small incision point, without risking pressure or puncture on the cortical surface beneath the dura. Flaps of dura were then peeled back with fine forceps or spring scissors to expose the brain surface, and tucked away around the edges of the craniotomy. Intracortical injections were performed after the duratomy while the entire area was submerged in sterile saline (see Viral Injections).

A window composed of stacked glass coverslips (four to five 4mm, plus one 5mm, Warner Instruments) bound with optical adhesive (Norland No.71) was then placed over the brain surface. The remaining saline was partially absorbed out with sterile eye-spears, and the craniotomy was sealed with cyanoacrylate glue (Vetbond, 3M) over a thin layer of sterile saline. Post-operative animals were administered buprenorphine (0.01-0.05 mg/kg) and carpofen (5 mg/kg) daily for 5-7 days following the surgery.

\subsection{Viral injections}
Intracortical injections were made at multiple sites ($\sim$5-9 sites per cranial window, spaced 0.5-1mm apart) using a microinjector (NanoFil, World Precision Instruments) fit with a 36G beveled needle (NF36BV-2, WPI). A total of $\sim$500-750nl was injected per site at a constant rate of 10-25nl/min at a depth of 750$um$ below the surface. A high-titre solution of viral vector (AAV9-syn-jGCAMP7f-WPRE, Addgene) was diluted to a final ratio of 2:1 with a 20\% mannitol solution (Sigma-Aldrich) to promote diffusion. Trace amounts of Fast-Green (Sigma-Aldrich) were added for visual confirmation of injected solution in the brain (see Figure\ref{fig:surgery_steps}). The exposed brain surface remained submerged in sterile saline throughout the injections.

% ----------------------------------------------------
% Area identification
% ----------------------------------------------------
\section{Wide-Field Mapping}
\subsection{Animal preparation}
About 20 minutes prior to the mapping session, animals were anesthetized with isofluorane (5\% induction, 1-1.5\% maintenance) and administered a subcutaneous dose of cholorprothixene (2 mg/kg, Sigma-Aldrich). During the mapping session, animals remained lightly anesthetized with minimal levels of isofluorane (0.5-1\%). Anesthesia levels were tested with the paw-pinch reflex and breathing rate. The left eye facing the monitor was checked between trials to ensure it remained open and stable. 

\subsection{Tandem-lens macroscope}
Widefield mapping was done with a tiltable, tandem-lens macroscope\cite{Ratzlaff1991, Kalatsky2003}, composed of a USB 3.0 CCD camera (MantaG033-B, Allied Vision) and 2 Nikon lenses (Nikon, 105-mm and 55-mm). Images were acquired at 25 Hz with 3x3 pixel binning (256x492 pixel ½-type sensor). Epifluorescence illumination was achieved with a 470 nm LED (Thorlabs) that was filtered and reflected through a filter cube that housed an excitation filter, dichroic mirror, and emission filter (Thorlabs). Green fluorescence or reflected light was collected and passed through the filter cube then focused on a CCD detector. For epifluorescence illumination, we used a 470 nm LED filtered and reflected by a long-pass dichroic mirror, and emitted fluorescence was filtered and captured at an imaging rate of 25Hz using custom Python scripts.

% Compare w. Wekselblatt et al. 2016:  Camera lenses allow a relatively high numerical aperture (NA) for light collection, which can also be adjusted easily using the f-stop setting to restrict the NA. This permits a flexible trade-off between sensitivity and depth of field, especially as increased depth of field is useful, given the curvature of the cortical surface. Imaging was generally performed at an f-stop of 5.6. The ratio of the focal lengths of the two lenses determines image magnification. To map 1 cm of cortex across the 2-cm detector (6.5 μm pixels), we chose 50 mm and 105 mm lenses, yielding magnification of 2.1× and 3.1 μm specimen pixels. In practice, we find an effective spatial resolution of ∼25 μm, based on the highest spatial frequencies present in nonbinned images of vascular structure. Binning across spatially oversampled pixels can reduce shot noise by allowing more total photons to be detected with increased illumination or NA. This is a standard practice in intrinsic signal imaging (Kalatsky and Stryker 2003) and is generally applicable at high light levels, where readout noise is negligible compared with photon count noise.

\subsection{Visual stimulation}
Visual stimuli were presented using custom Python scripts on a 72 inch LCD monitor (LG). The monitor was centered in front of the left eye, spanning 177 degrees of visual field along azimuth, 67 degrees along elevation. A periodic stimulus consisting of a bar sweeping across the screen \cite{Kalatsky2003, Marshel2011} was presented to the (left) eye contralateral to the cranial window. The bar subtended 5 degrees of visual angle, and was either presented as a white bar sweeping over a black background or an apertured bar, containing one of 32 possible natural scene images, drifting over a gray background. The bar was presented at 0.13 Hz along the azimuth and elevation axes, for a total of 2 (downward, rightward) or 4 (downward, rightward, leftward, upward) conditions. The selected stimulation frequency was one of a subset tested that avoided frequency ranges of known, non-stimulus-driven, physiological signals (\textit{e.g.}, heart rate or breathing rate). One trial consisted of 10 cycles of the bar, and a total of 4-5 such trials were acquired for each direction. To preserve the speed of the bar between azimuth and elevation conditions, the bar traversed the full extent of the monitor's width centered along the monitor's vertical extend (bar started and ended off screen). 

\subsection{Image processing}
Raw fluorescence signals were corrected for slow drift by removing the rolling average of each pixel’s time course. The width of the rolling window was set to 2.5 times the length of the stimulation period to remove slow linear and non-linear trends. For each pixel, the time courses for each trial (10 cycles of the stimulus moving along a given direction) were aligned and averaged for each condition (1 of 4 possible directions). We then performed a Fourier spectral analysis on the averaged time series for each pixel to get a magnitude and phase value for each pixel at each frequency. The strength of the response to the stimulus was calculated as the ratio of the Fourier magnitude at the frequency of stimulation to the sum of the magnitudes at all other frequencies\cite{Kalatsky2003, Marshel2011, Juavinett2017}.

\subsection{Area segementation}
Retinotopic maps were created by taking the phase values for all pixels in the image and converting them to Cartesian coordinates that matched the linear position of the bar on the monitor to the phase of the stimulus cycle that corresponded to that position. To identify the borders between visual areas, maps of vertical and horizontal retinotopy were combined to calculated a visual field sign map\cite{Juavinett2017, Zhuang2017}. The visual field sign was computed by taking the sine of the difference between the vertical and horizontal retinotopic gradients at each pixel. Sign map were then filtered and thresholded to reveal key visual areas:  areas with mirror representations map to 1 (blue) and areas with nonmirror representations map to -1 (red), as shown in Figure\ref{fig:retino_mapping}. 

While multi-site viral injections allowed for greater coverage of exposed cortex, they also resulted in patchy expression levels in a subset of animals. Animals with ambiguous retinotopic maps were excluded from further study. We followed previous systematizations for area identification\cite{Juavinett2017} and used a combination of relative location to the other identified areas, relative cortical area, and visual field sign. For visualization, phase and power maps were smoothed with a Gaussian window (FWHM=$50um$) and masked by applying a threshold to the magnitude ratio.  

% ----------------------------------------------------
% 2p imaging
% ----------------------------------------------------
\section{Two-photon calcium imaging}
A battery of stimuli were used to characterize responses across rat lateral visual cortex. Briefly, a moving bar stimulus was used to map two-photon retinotopic preferences and register two-photon fields-of-view to wide-field maps identified with the same stimulus paradigm. To measure more fine-scaled receptive field properties, a standard tiling stimulus was used to estimate the position, extent, and shape of receptive fields. To estimate responses to simple features such as edge orientation or direction tuning, a set of drifting gratings were used. Finally, to characterize more complex object representation, as tested in the behavioral choices of trained rats, a subset of the same object stimuli were used to measure neuronal responses in naive rats. While localizer runs only used the moving bar and receptive field protocols, subsequent functional runs tested the full suite of stimuli.  

\subsection{Visual stimulation}
Head-fixed animals passively viewed visual stimuli presented on an LCD monitor (LG, 1920x1080 resolution, 60 Hz refresh rate), subtending \ang{120} of visual angle along azimuth and \ang{67} degrees along elevation axes of the visual field contralateral to the cranial window (see Figure\ref{fig:2p_retino}). Stimulus presentation was synced to neural data acquisition for each trial using custom software (MWorks, Python). 

\subsubsection{Moving bar}
A white bar subtending 2-5 degrees of visual angle cycled across a black screen at a stimulation frequency of 0.24Hz or 0.13Hz. Each of four cardinal directions were tested (downward, upward, leftward, rightward). Each trial consisted of of 12 cycles of the stimulus, and 4-6 trials were presented for each of the four conditions, totalling 16-24 trials total. Blank trials were also included to measure baseline fluctuations in spontaneous activity.  

\subsubsection{Receptive field mapping}
To estimate receptive field size and positions, we adapted a standard mapping protocol~\cite{Marques2018}. The monitor was subdivided into square tiles, in which a square-wave, drifting grating was presented at a spatial frequency of 0.25 cycles/deg, and driting speed of 10 cycles/sec. On each trial, the drifting grating randomly switched every 125msec between 4 cardinal directions. Each tiled position was stimulated for 500ms, followed by 1 sec inter-trial interval (ITI). Since the receptive field mapping used a shorter ITI, we restricted the stimulated position of a given trial to be a minimum of 30 degrees of visual angle away from the position of the previous trial. 
The size of the square aperture, \textit{i.e.}, the tile, was either 5 degrees of visual angle or 10 degrees of visual angle. For the small tile (5 degrees), this resulted in 21 positions along azimuth and 11 positions along elevation, totalling 231 positions. Each position was stimulated a minimum of 10 times total across 5 blocks, with 2 repetitions of each of 231 positions per block. For the larger tile (10 degrees), this resulted in 11 azimuth positions and 6 elevation positions, totalling 66 positions. Each position was stimulated a minimum of 10-20 times total across 2-4 blocks, with 5 repetitions of each of 66 positions per block. Since the larger tile required significantly fewer conditions than the smaller tile, it was used for most localizer runs with more reptitions (20, instead of 10) per position. 

\subsubsection{Drifting gratings}
To measure visual feature tuning, we presented drifting, square-wave gratings at 8 directions (ang{0} to \ang{315}, steps of \ang{45}). Gratings were presented at either full screen or within a circular aperture whose size was determined by the average receptive field size of the population recorded in previous localizer sessions. All gratings were also presented at two spatial frequencies (0.5 and 0.1 Hz) to target the low and high end of known visual acuity levels~\cite{Prusky2000}, and at two speeds (10 deg/s and 20 deg/s). This set of stimulus configurations resulted in 64 unique grating stimuli, which were repeated $\sim$20 times in pseuedo-random order across 4 blocks, such that each block contained 5 repetitions of each of the 64 conditions. Gratings were presented on a gray background (luminance-matched) for 500msec, followed by 2sec inter-trial intervals of blank gray screens. 

\subsubsection{Objects}
Visual objects were renderings of three-dimensional models built using a ray tracer package POV-Ray (http://www.povray.org/). Each object was defined as a particular configuration and blend of spheres. The particular objects selected for the anchors were modeled to replicate the stimuli used in a previous study\cite{Zoccolan2009}. Figure\ref{fig:basic_training} shows the ``default'' object views used during phase 1 of training. Objects were rendered with the same light source location and matched to have approximately equal height, width, and area, as defined by the a bounding box surround each object rendering. Object transformations (\textit{e.g.}, size, in-depth rotation) were generated using custom Python wrappers and the POV-Ray API). Morph stimuli were generated by gradually adjusting the relative proportions of each object, by parametrically shifting the spheres defining one object into the spheres defining the other. We used the Euclidean distance in pixel space to quantify the difference between each neighboring pair of images. 

For two-photon imaging experiments, a subset of these morph stimuli were used: Each morph (7 intermediate morphs, plus 2 anchors) was presented at 5 different sizes (10-50 degrees, in 10 degree steps). For each size, mean luminance was measured with a photometer to determine the levels needed to create full-screen controls matched for each size and object. This resulted in 50 unique object conditions, each presented a minimum of 30 times across 6 blocks, with 5 repetitions of each of the 50 condition per block. Stimuli were presented for 1sec, followed by a 2 sec inter-trial interval. 


\subsection{Neural Data Acquisition}
% ScanImage vX.X (REFREF)
% Volumetric rate for anatomicals REFREF
Neural imaging data was collected using a custom-built galvo-resonant scanning two-photon microscope (20 kHz; Cambridge Technologies) and a 0.8 WD/16x objective lens (CF 175, Nikon Technologies). Light shielding around the objective was used to block light emitted from the LCD monitor. The microscope body was tilted at an angle of 40 degrees to match the plane of the cranial window. We used 920 nm excitation for both channels using a Ti:Sapphire laser (80 Mhz, MaiTai-eHP DeepSee, prechirped, Spectra-Physics). Laser power measured at the object was 50-80mW, likely an overestimate of actual power reaching the tissue through the cranial window. Single plane images were collected at a rate of 44.65 Hz (512x512 pixels; 1$mm$ x 1$mm$ FOV) using ScanImage (Matlab, Vidrio Technologies). 

For anatomical volumes, a 200$um$ z-stack was taken in steps of 20$um$, at a volumetric rate of X Hz, simultaneously for both channels. A total of 100 volumes were taken and averaged for anatomical images. For localizer and functional runs (see below), the FOV was placed 150-300 um below the bottom layer of cranial window or the surface blood vessels. 
%%

\subsubsection{Localizer Sessions}
A set of target candidate FOVs were identified in each animal with a viable window (clear cellular resolution, identified visual areas). Given the large suite of stimuli included in the survey and to maximize repeitions per condition, localizer sessions were run for each FOV to validate that the target FOV was in the visual area of interest, and to identify the best stimulus location to present localized, or non-full screen, stimuli for the objects and gratings blocks. For a given FOV, the targeted stimulus location for functional sessions was determined by taking the center of mass across all cells' receptive fields. As an additional constraint, only FOVs for which the bounding box around the largest sized stimuli were within the monitor's bounds were accepted for functional sessions.

\subsubsection{Functional Sessions}
Since the kinematic mounts allowed the same cells to be identified across days (see Figure\ref{fig:multiday_imaging}), it was possible to return to the same FOV that was mapped in a previous localizer session (usually mapped the previous day). Localized stimuli (drifting gratings and objects) were then centered at the target stimulus coordinates identified in the localizer sessions. In addition, both the moving bar retinotopic mapping paradigm and the receptive field mapping paradigm, both used in the initial localizer session, were repeated in the functional sessions. This allowed us to acquire receptive field estimates in the same cells and the same sessions in which object and gratings responses were collected, while avoiding the concern that different cells might be active during functional sessions than localizer sessions, or that receptive fields might subtly change from one day to the next.

\subsection{Behavior Acquisition}
High-resolution images of the animal's behavior state were acquired in sync with neural data acquisition using custom Python software. A CMOS camera (Manta G-033, Allied Vision) with a zoom lens was centered on the animal's head and upper body on the side facing the monitor, illuminated with an IR LED. Images were acquired at 20Hz. 
% Image size, resolution REFREF
% camera/sensor info
% Lens/LED info?

\subsection{Image Processing}
Motion correction, ROI selection, neuropil correction, and trace extraction were done with a custom pipeline written in Matlab and Python. Motion correction used rigid transformations within each FOV using custom Matlab code (Chris Harvey, Harvard Medical School). 
\subsubsection{Cell Mask Identification}
For ROI selection, an activity map was created by taking the standard deviation across motion-corrected frames within a movie file for each block of trials, and then taking the maximum projection across all blocks. ROIs were selected manually with a circular mask using a custom Matlab GUI. To remove background calcium signals, we estimated neuropil masks as circular annuli of 11$um$ width, with the inner edge at 9$um$ beyond the outermost edge of a corresponding cell body and the outer edge extending to 20$um$. Pixels from adjacent cell body masks were excluded from the neuropil masks. 
\subsubsection{Time Course Extraction and Correction}
To get raw fluorescent traces for a given mask, the fluorescence intensity of a cell at each time point was computed as the average fluorescence across pixels within the mask. To correct for slow drift effects due to long imaging sessions, a correction procedure was applied. First, a baseline $F_0$ signal was extracted with a sliding filter (20\% percentile of a 30 sec sliding window) for each cell in each movie. For each trace, this drift was subtracted from the raw trace before adding back the mean of the offset value. 
To account for neuropil signals that could contaminate the soma trace, neuropil correction was applied as follows: $F_{soma}$(t)=($F_{raw}$(t) - $F_{neuropil}$(t)) + $\overline{F}_{neuropil}$, where $t$ is time, $r$ is an empirical constant defined as the ratio $F_{bloodvessel}$/$F_{neuropil}$ (set to 0.7), and $\overline{F}_{neuropil}$ is the temporally-averaged mean of the neuropil fluorescence\cite{Liang2018, Kerlin2010}. For a subset of data, both Suite2p and CaImAn were tested.

Fractional change in fluorescence, $\Delta$F/F(t), following visual stimulus presentation was calculated as: $\Delta$F/F(t)=(F(t)-F$_0$)/F$_0$, where F(t) is the corrected fluorescence trace during the stimulus presentation flanked by 1 sec before and after stimulus onset and offset, and F$_0$ is the 1 sec baseline period prior to stimulus onset. Single response values for a given trial were obtained by averaging the $\Delta$F/F(t) response during the stimulus presentation window x 2. The additional factor was to account for cells that exhibited OFF-type responses or slow response kinetics. 

\subsection{Area Identification and Validation}
Each two-photon FOV was coregistered to wide-field retinotopic maps using blood vessel markers. All two-photon imaging sessions began with the acquisition of an anatomical volume, which was a  500$um$ z-stack taken from the surface. Prior to the start of the imaging session, rats were given subcutaneous injections of SR101 (Sigma-Aldrich) for fluorescent labeling of the blood vessels visible in the red channel. 

Two-photon blood vessel images were matched to wide-field maps offline. Matching points between the two images were selected based on uniquely identifiable blood vessels, then used to identify a transformation matrix to warp one image into the other. Assignment of two-photon FOVs were validated based on retinotopic maps measured with the same cycling bar paradigm used for wide-field maps of azimuth and elevation. Sign maps were obtained from the retinotopic maps with a series of morphological filters\cite{Marshel2011, Garrett2014, Zhuang2017}, which were then used to identify patches representing putative visual areas. Two-photon FOVs were segmented based on the visual field sign maps (see Wide-Field Mapping, Area Segmentation). Since a given FOV could contain more than one visual area, cells were assigned based on both the segmented two-photon sign maps and the wide-field sign maps. Only two-photon FOVs with matches to wide-field maps and unambiguous sign reversals were included for subsequent analyses. 

\subsection{Pose Extraction}
% DLC
% training parameters (DLC, v2)
% manual selection


% ----------------------------------------------------------
% Retintopy/Receptive fields
% ----------------------------------------------------------
\subsection{Estimation of retinotopic tuning preferences}
\subsubsection{Retinotopic preferences of background neuropil}
The center of each neuropil ring was first assigned a value corresponding to the preferred retinotopic location of the neuropil ring (average over all pixels within the ring). The center was then dilated to a disk of 20$um$ radius, averaging the preferred retinotopy for overlapping disks. Pixel-wise estimates of retinotopic preference were obtained by smoothing the resulting dilated and averaged image with an isotropic two-dimensional Gaussian filter with standard deviation of 7$um$. 

For each FOV, the spatial axis corresponding to the direction of maximal retinotopic change for a given retinotopic axis (elevation or azimuth) was identified as follows\cite{Liang2018}. First, the two-dimensional pixel-wise gradient was calculated as:  
\begin{align}
\nabla Ret(x, y) = (\partial Ret(x, y)/\partial x)\hat{i} + (\partial Ret(x, y)/\partial y)\hat{j}.
\end{align}
The spatial axis was then computed as the normalized average gradient vector, $\overline{\nabla Ret}/ \|\overline{\nabla Ret}\|$, across all pixels. The smoothed neuropil map was then projected onto this mean gradient vector, such that for each pixel, its projected location along this new spatial axis was defined as:  $\tilde{x}=(x, y)\cdot \overline{\nabla Ret}/ \|\overline{\nabla Ret}\|$. The relationship between a pixel's preferred retinotopic location (based on the smoothed neuropil maps) and $\tilde{x}$ was modeled with a linear function, $R_{fit}=a\tilde{x}+b$, where $a$ is the fit parameter (in visual field degrees/cortical $um$) that corresponds to the rate of retinotopic progression along the map. The normalized mean gradient and linear fit were computed separately for azimuth and elevation.

\subsubsection{Estimation of receptive fields}
For each ROI, responses at each stimulated location were baseline subtracted (0.5 s before stimulus onset), then averaged across repetitions\cite{Marques2018}. A MxN stimulus response map was computed by averaging the responses during the stimulus window and 0.5 s after stimulus offset (1 s total). The response map $R(az, el)$, where $az$ and $el$ are the retinotopic coordinates in azimuth and elevation, respectively, was then fitted with a two-dimensional Gaussian curve, using the Python implementation of the least-squares Levenberg-Marquardt algorithm\cite{More1978, Virtanen2020}:
\begin{align}
%\[
R(az, el) = a +b*\exp\left[ -\left( \frac{(az-az_0)\cos\theta + (el-el_0)\sin\theta}{\sqrt{2} \sigma_1} \right)^2-\left( \frac{-(az-az_0)\sin\theta+(el-el_0)\cos\theta}{\sqrt{2}\sigma_2} \right)^2 \right] 
%\]
\end{align}
where ($az_0$, $el_0$) is the center of the 2D Gaussian in azimuth and elevation, $\sigma_1$ and $\sigma_2$ are the standard deviations along the two axes of the Gaussian, $\theta$ is the angle of the Gaussian relative to the azimuth-elevation coordinate system, and $a$ and $b$ are offset and amplitude parameters, respectively. 

The receptive field boundary was considered to be the ellipse defined by the center ($az_0$, $el_0$) and standard deviations ($\sigma_1$, $\sigma_2$) of the fitted Gaussian:

\begin{align}
\left[ \frac{(az-az_0)\cos\theta + (el-el_0)\sin\theta}{\sigma_1} \right]^2 + \left[ \frac{-(az-az_0)\sin\theta + (el-el_0)\cos\theta}{\sigma_2} \right]^2 = 1.
\end{align}

Only fits with $R^2$>0.5 and s.d. between \ang{2.5} and \ang{55} were included for further analyses. To determine whether the fitting procedure yieled a high-quality, reliable fit, we used a bootstrap procedure to estimate confidence intervals (95\% CI) for each estimated parameter. Specifically, for each of 1000 iterations, 10 trials per condition were sampled with replacement, then averaged and fitted according to the procedure described above. This generated a distribution of parameter estimates, which were then used to determine the 95\% CI for each cell's estimated RF parameters. Only cells with fits lying within the 95\% CI ($az_0$, $el_0$, $\theta$, $\sigma_1$, and $\sigma_2$) were included.

\subsubsection{Estimation of retinotopic scatter}
For each FOV, retinotopic scatter was estimated as the deviation, $D_{VF}$, in degrees of visual field space, from the predicted receptive field center based on its estimated cortical position:
$D_{VF}=|Ret_{pref}(\tilde{x}) - Ret_{fit}(\tilde{x})|$, where $Ret_{pref}(\tilde{x})$ denotes the cell's measured receptive field center, and $Ret_{fit}(\tilde{x})$ denotes its predicted retinotopic preference according to its projected location $\tilde{x}$ along the mean gradient axis. Cortical scatter, $D_{CX}$, in microns, was calculated as the absolute deviation from the predicted cortical position based on the spatial progression defined by the gradient axis: $D_{CX}=|\tilde{x}-(Ret_{VF}(\tilde{x})-b)/a|$, which corresponds to the distance (along the spatial gradient axis) a given cell would have to move along azimuth or elevation in order to form a perfectly progressing smooth retinotopic map. 


% ----------------------------------------------------------
% GRATINGS
% ----------------------------------------------------------
%\section{Estimation of tuning preferences}
\subsection{Tuning preferences of cells responsive to drifting gratings}
For each cell and each stimulus condition, we determined the evoked response to be significantly different from noise if the amplitude of dF/F during the stimulus period exceeded 2.5 standard deviations above or below the mean baseline activity (computed using 0.5s or 1s prior to stimulus onset) for at least 10 out of the 44 time points (44.65 Hz frame rate * (0.5 s stimulus presentation + 0.5 s after stimulus offset)).

To determine if a given cell showed a significant response at a particular stimulus configuration (a unique combination of size, speed, spatial frequency), we required that at least 2 out of the 8 directions at this given stimulus configuration evoked responses according to the above criteria. Axis selectivity, direction selectivity, and preferred theta were calculated following previously published metrics\cite{Liang2018}. 

\subsubsection{Direction tuning curve fitting}
For each cell that exhibited a significant response at a given combination of spatial frequency, size, and speed, a bootstrap analysis (1000 iterations of 20 samples each, to match the measured sample of gratings trials) was performed to compute direction tuning curves. Direction tuning curves were originally sampled in steps of \ang{45}. To obtain a more precise estimate of tuning parameters (preferred orientation and direction), tuning curves were fit with a two-peaked Gaussian\cite{Liang2018, Sun2016}:

\begin{align}
R(\theta) = R_1 \exp{ - \frac{-(\theta-\theta_{pref})^2}{2\sigma^2} } + R_2 \exp{ - \frac{-(\theta-\theta_{pref}-\ang{180})^2}{2\sigma^2} } + R_{offset},
\end{align}

where $R(\theta)$ is the $\Delta$F/F(t) response for gratings direction $\theta$, $\theta_{pref}$ is the direction evoking the strongest $\Delta$F/F(t) response, $R_1$, $R_2$ is the amplitude of the second peak at $\theta_{pref} + \ang{180}$, and $R_{offset}$ is a constant amplitude offset. The model assumes the two peaks of the Gaussian are \ang{180} apart, and that the Gaussians have a common standard deviation, $\sigma$. Initial parameters were set as follows:  $\sigma$ was initialized as the step size (\ang{45}), $R_{offset}$ was the mean of responses to the null directions (all directions except for $\theta_{pref}$ and $\theta_{pref}+\ang{180}$). 

To improve the reliability of the fitting and the accuracy of estimated preferred direction and orientation, several additional steps were implemented by following a simplification of the procedure outlined in \cite{Liang2018}. First, we added a ninth point at \ang{360} by copying the point at \ang{0} to wrap the input values. Then, the number of input points was increased from 9 to 25 by linearly interpolating the 9-point tuning curve, to more finely sample the curve for the fitting procedure.  

A bootstrap procedure was used to fit the tuning curves. On each iteration, 20 trials were randomly sampled (with replacement) for each of the 64 conditions, then averaged, interpolated, and fit according to the steps outlined above. The final tuning curves for each cell (one for each unique combination of spatial frequency, size, and speed to which the cell was significantly responsive) were computed from the mean of the fitted parameters across the sampling iterations. 

To evaluate the quality of fits, a combination of criteria were used\cite{Liang2018}. For each iteration of the fitting procedure, a coefficient of determination, $r^2$, was calculated as the explained variance using least-squares regression to fit the data\cite{More1978, Virtanen2020}. A combined coefficient of determination, $r_{comb}^2$, was also calculated for the original tuning curve versus a fitted curve derived from the average of each fit parameter (across the 1000 iterations). These metrics were combined into goodness-of-fit heuristic, G$_{fit}$: 
\begin{align}
G_{fit}=\overline{\sqrt{r^2}} (1-IQR_{\sqrt{r^2}}) \sqrt{r_{comb}^2}, 
\end{align}
where $IQR_{\sqrt{r^2}}$ is the interquartile range (difference between the $25^{th}$- and $75^{th}$-percentiles of $\sqrt{r^2}$ values across iterations. A cell was considered to have a well-fit tuning curve at a given combination of spatial frequency, size, and speed if its $G_{fit}$ was greater than or equal to 0.5. 

\subsubsection{Axis and direction selectivity}
For each cell exhibiting significant responses to gratings (see above), a vector sum axis selectivity index (ASI) was computed as a metric for selectivity of motion along a given axis\cite{Liang2018, Kerlin2010}. Axis selectivity is distinguished from orientation selectivity, as the latter is typically measured with static gratings, which the current study did not test. To calculate the ASI, the responses for each of the directions was projected onto a circle with $2i$ progression and the magnitude of the normalized vector sum was estimated according to:  $ASI=\big|\sum_{i=1}^{24} R(\theta_i) \exp^{2i*\theta_i}\big|/\sum_{i=1}^{24}|R(\theta_i)|$. ASI values ranged from 0 (no selectivity) to 1 (maximum selectivity). Opposite directions were additive, while orthogonal directions canceled each other out. The ASI was computed for 1000 iterations using the same bootstrap procedure as used in calculating the tuning curves. For each cell, the final ASI was computed as the mean ASI across all combinations of spatial frequency, size, and speed for which the cell exhibited significant evoked responses. 

A direction selective index (DSI) was computed in a similar way, the responses were projected onto a circle with $1i$ progression:  $DSI=\big|\sum_{i=1}^{24} R(\theta_i) \exp^{1i*\theta_i}\big|/\sum_{i=1}^{24}|R(\theta_i)|$. DSI computations were iterated 1000 times, with the final DSI value for a cell taken as the mean DSI across stimulus combinations that evoked significant responses. 

A cell was determined to be direction-selective if a) it had a significant response to the grating stimuli (see ``Estimation of cells visually responsive for drifting gratings''), b) it had a well-fit direction tuning curve (see ``Direction tuning curve fitting''), and c) it had an average direction selectivity index (DSI) greater than or equal to 0.2\cite{Liang2018} for all stimulus combinations to which it had a significant response. Similarly, a cell was considered to be axis-selective if a) it had a significant response to the grating stimuli, b) it had a well-fit direction tuning curve, and c) its ASI exceeded 0.15 and its DSI was less than 0.2 for all stimulus combinations for which a significant response was observed.

\subsubsection{Estimation of preferred direction}
For direction-selective cells, preferred direction of motion was determined by taking the circular average of the fitted $\theta_{pref}$ across stimulus combinations for which a significant response was observed, and direction tuning curves passed goodness-of-fit thresholds. 

\subsubsection{Preference Metrics for Spatial Frequency, Size, and Speed}
For cells exhibiting significant visual responses to the drifting gratings, a preference index was calculated for each non-direction parameter, \textit{i.e.}, spatial frequency, size, and speed. Each non-direction parameter was tested at two levels, one low and one high, and the preference index were defined as:  $(R_{low}-R_{high})/(R_{low}+R_{high})$, which ranged from -1 (preference for lower) and 1 (preference for higher). For each cell, a bootstrap procedure was used to determine whether each preference index was significantly different from zero. Specifically, a preference index was calculated for each of N iterations by sampling trials (with replacement) for each of the 64 conditions, creating a distribution of preference index values. The cell's preference index was considered significant for that parameter if the 95\% CI did not contain 0 (no preference). To compare the distribution of preference indices across visual areas for a given parameter (spatial frequencty, size, or speed), only cells with a significant preference index for that parameter were included.

%%%% 
% ----------------------------------------------------------
% Objects
% ----------------------------------------------------------
\subsection{Tuning preferences of cells responsive to objects}

\subsubsection{Estimation of cells with significant visual responses to objects}
For each cell, stimulus evoked responses were determined to be significant using a receiver-operating-characteristic (ROC) analysis from signal detection theory\cite{}. For each cell, a distribution of stimulus responses and a distribution of baseline responses were obtained for each stimulus condition (one response value per trial). For each condition, an ROC curve was obtained across a range of criterion levels (50 levels, linearly sampled between the cell's minimum and maximum stimulus response values) by calculating the proportion of times the stimulus and baseline responses exceeded a given criterion level. The area under the ROC curve (AUC) was calculated for each condition, such that a value of 0.5 corresponds to fully overlappying stimulus and baseline response distributions, while increasingly larger values indicate increased separability between the stimulus and response distributions. The maximum AUC value for a given cell thus corresponded to the stimulus condition for which the cell's responses were maximally distinguishable from baseline responses. For each cell, the significance of its maximum AUC value was evaluated by a shuffle test:  for each of 1000 iterations, the baseline and stimulus labels were shuffled, ROC curves and AUC values were calculated, and the likelihood of the cell's maximum AUC was determined as the fraction of times the true AUC was greater than the AUCs derived from the shuffled labels. Only cells with significant AUCs (p<0.05) were included as visually responsive. 

\subsubsection{Selectivity and tolerance metrics}
Neuronal selectivity to morphs was quantified by a morph tuning index\cite{Zoccolan2007}:
\begin{align}
MT=[n-(\sum_{i=1}^{9}R_i/R_{max})]/(n-1), 
\end{align}
where $R_i$ is a neuron’s response to the $i^{th}$ morph ($n=9$ morphs), $R_{max}$ is the maximum response amongst the morphs. As a measure of response sparseness, MT ranges from 0 (no shape selectivity) to 1 (maximally shape selective). Size tolerance was quantified by normalizing size tuning curves to their maximum values, then averaging those resulting values that were < 1:  $ST=<R_{test}/max(R_{test})>$, where $R_{test}$ is the mean response to a given stimulus size of a neuron’s most preferred object, and $<.>$ denotes the average across tested stimulus sizes where $R_{test}<max(R_{test})$\cite{Zoccolan2007}. 


\subsection{Single-cell Discriminability}
% DLC
% training parameters (DLC, v2)
% manual selection


\subsection{Arousal modulation}
% DLC
% training parameters (DLC, v2)
% manual selection


%\subsection{Population readout}
\subsubsection{Population Discriminability}
To quantify discriminability, we trained linear classifiers (support vector machines) to discriminate the two original objects from the neural activity in each area. The linear-readout scheme is important in that it is a simple, biologically plausible processing step that amounts to a thresholded sum taken over weighted synapses. This classifier approach does not provide a measure for the total information present in the population, but rather estimates the lower bound on the information explicitly accessible to the population to support the visual task\cite{Hung2005, Rust2010SelectivityIT}.

Linear support vector machines (SVMs) were trained to discriminate object A from object B from neural responses. Each presentation of an image produced a population response vector \textbf{x} of size $Nx1$, such that repeated presentations form a cluster of points in $N$-dimensional space (see Figure\ref{fig:neural_generalization}). The linear readout amounts to finding a linear hyperplane that best separates the response clouds corresponding to each image from those corresponding to all other images. Specifically, the linear readout amounts to\cite{Rust2010SelectivityIT, Hung2005, Li2009}: 
\begin{align}
f(\textbf{x})=\textbf{w}^T\textbf{x} + b, 
\end{align}
where \textbf{w} is a $Nx1$ vector of the linear weights applied to each of $N$ neurons (defines the orientation of the hyperplane), and $b$ is a scalar that offsets the hyperplane from the origin. To determine the population’s choice about which image was presented, a response vector \textbf{x} (population response to one image) was applied to the classifier, and negative values of f(\textbf{x}) indicate object A and positive values indicate object B. Performance was defined as the proportion of correct answers when asked to classify each image with a held-out test set never included in training. 

The hyperplane and threshold for each classifier was determined by a support vector machine (SVM) using the scikit-learn machine learning library (LinearSVC\cite{Pedregosa2011}). The data were split into train and test sets (20\%) after balancing numbers of samples per condition. We used a 5-fold cross-validation procedure on the train set to fit and evaluate each model:  of the trials partitioned for training, models were optimized where the best $C=\{10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 10^2, 10^3\}$ that yielded the highest accuracy was chosen. Performance was then assessed with the held-out test set. 

\subsubsection{Linear separability and generalization}
To test linear separability (see Figure\ref{fig:neural_generalization}), 80\% of the trials corresponding to object A and object B for each size were simultaneously combined to train and evaluate the models, while the remaining 20\% of trials was used to measure classifier performance. To test generalization, two regimes were used. In the first, ``Train one, test one'' (Figure\ref{fig:neural_generalization}B-D), each classifier was trained to classify object A and B at one of the 5 sizes, then tested on each of the remaining 4 untrained sizes. Each training set (for each size) included 80\% of the trials for a given size, while the test sets could contain either the remaining 20\% of trials of the same size (test accuracy on ``trained'' conditions) or 100\% of the trials at one of the other sizes (test accuracy on ``novel'' conditions). 

In the second regime, ``Train a subset, test one'' (Figure\ref{fig:neural_generalization}),  each classifier was trained with trials of  4 of the 5 sizes together, then tested with the remaining, untrained size. Each training set was composed of 80\% of trials at each of the 4 train sizes. The remaining 20\% of trials for each of the 4 train sizes combined to form the held-out test set (test accuracy on ``trained'' conditions), while 100\% of the trials of the remaining 5th size formed the other type of test (test accuracy on ``novel'' conditions). All combinations of 4 of 5 sizes were used to train different classifiers and test generalization performance on samples of the remaining fifth size.

\subsubsection{Population sampling}
In analyses in which a given metric, \textit{e.g.}, classifier accuracy, is presented as a function of the number of neurons in a pseudo-population, we applied a resampling procedure to measure the variability that can be attributed to the particular subpopulation of neurons or particular subset of trials used for training versus testing. On each iteration, we sampled a new subpopulation of neurons that were randomly selected (without replacement) from all cells aggregated across imaging sites and animals, for a given visual area, and trials were randomly assigned for training and testing (without replacement). Error bars were calculated as the SD of classifier performance across 100 iterations. Chance performance was computed by randomly assigning objects or images associated with each response vector and repeating the classification analysis.


\section{Quantification and statistical analysis}
For all pairwise tests, Wilcoxon signed-rank tests were used, unless otherwise specified. Significance values were set to p<0.05 or p<0.01. For all comparisons between visual areas, Mann-Whitney U-tests, with significance values corrected for multiple comparisons.
